#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\renewcommand{\listtablename}{Indice de tablas}
\renewcommand{\tablename}{Tabla} 
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 1.5cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Machine Learning
\end_layout

\begin_layout Standard
En esta sección se realiza un breve repaso sobre conceptos básicos que envuelven
 a Machine Learning.
 El objetivo es visualizar qué aspectos de Machine Learning fueron tomados
 como componentes de solución al problema de estudio.
\end_layout

\begin_layout Section
Definición
\end_layout

\begin_layout Standard
En 1959 Arthur Samuel en una publicación escribió: 
\shape italic

\begin_inset Quotes eld
\end_inset

Programming computers to learn from experience should eventually eliminate
 the need for much of this detailed programming effort
\begin_inset Quotes erd
\end_inset


\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Arthur1959"

\end_inset

.
 Este pionero de machine learning ya presagiaba que los programas, a partir
 del aprendizaje sobre datos históricos (la experiencia), podrían efectuar
 tareas de toma de decisiones sin ser programadas explícitamente dichas
 decisiones.
\end_layout

\begin_layout Standard
Samuel define machine learning como sigue: 
\shape italic

\begin_inset Quotes eld
\end_inset

Machine Learning es un campo de estudio que da a las computadoras la capacidad
 de aprender sin ser explícitamente programadas
\begin_inset Quotes erd
\end_inset


\shape default
.
 Otro investigador de machine learning Tom Mitchell propuso en 1998 la siguiente
 definición: 
\shape italic

\begin_inset Quotes eld
\end_inset

Well posed Learning Problem: A computer program is said to learn from experience
 E with respect to some task T and some performance measure P, if its performanc
e on T, as measured by P, improves with experience E
\begin_inset Quotes erd
\end_inset


\shape default
.
 Donde se nos indica que el aprendizaje en las máquinas deberá ser parecido
 al aprendizaje en los humanos, por ejemplo cuando una criatura comienza
 a hablar a través de la experiencia de pronunciar las palabras y de su
 interacción con otras personas, entonces sucede que su capacidad de hablar
 se va perfeccionando o mejorando.
\end_layout

\begin_layout Standard

\shape italic
\begin_inset Quotes eld
\end_inset

The purpose of machine learning is to learn from training data in order
 to make as good as possible predictions on new, unseen, data
\begin_inset Quotes erd
\end_inset


\shape default

\begin_inset CommandInset citation
LatexCommand cite
key "Jean2016"

\end_inset

.
 La dificultad radica en que debemos construir modelos que nos acerquen
 a una buena predicción sobre datos aún no conocidos o imprevistos.
 Peter Prettenhofer y Gille Louppe presentan la siguiente definición:
\end_layout

\begin_layout Standard
Data comes as...
\end_layout

\begin_layout Itemize
A set of examples 
\begin_inset Formula $\left\{ \left(x_{i},y_{i}\right)\mid0\leq i<n\;samples\right\} $
\end_inset

, with
\end_layout

\begin_deeper
\begin_layout Itemize
Feature vector 
\begin_inset Formula $x\in\mathbb{R}^{n\;features}$
\end_inset

, and
\end_layout

\begin_layout Itemize
Response 
\begin_inset Formula $y\in\mathbb{R}$
\end_inset

(regression) or 
\begin_inset Formula $y\in\left\{ -1,1\right\} $
\end_inset

 (classification)
\end_layout

\end_deeper
\begin_layout Itemize
Goal is to...
\end_layout

\begin_deeper
\begin_layout Itemize
Find a function 
\begin_inset Formula $ŷ=f\left(x\right)$
\end_inset


\end_layout

\begin_layout Itemize
Such that error 
\begin_inset Formula $L\left(y,ŷ\right)$
\end_inset

on new (unseen) 
\begin_inset Formula $x$
\end_inset

 is minimal
\end_layout

\end_deeper
\begin_layout Section
Formas de Aprendizaje 
\begin_inset CommandInset citation
LatexCommand citet
key "IAEModerno2"

\end_inset


\end_layout

\begin_layout Standard
Los algoritmos de aprendizaje automático se pueden categorizar según la
 forma en que se realiza el aprendizaje, pero teniendo en cuenta que todos
 reciben un conjunto de ejemplos del que aprender.
\end_layout

\begin_layout Standard
El tipo de realimentación disponible para el aprendizaje normalmente es
 el factor más importante a la hora de determinar la naturaleza del problema
 de aprendizaje que tiene que afrontar el agente.
 Se distinguen tres distintos tipos de aprendizaje: supervisado, no supervisado
 y por refuerzo.
\end_layout

\begin_layout Subsection
Aprendizaje supervisado
\end_layout

\begin_layout Standard
El algoritmo recibe datos de entrenamiento que contienen la respuesta correcta
 para cada ejemplo.
 El problema de estudio utiliza algoritmos de aprendizaje supervisado, donde
 el experto en compras da la respuesta correcta a cada ejemplo.
\end_layout

\begin_layout Standard
El problema de aprendizaje supervisado consiste en aprender una función
 a partir de ejemplos de sus entradas y sus salidas.
 Los casos (1), (2) y (3) son ejemplos de problemas de aprendizaje supervisado.
 En el caso (1), el agente aprende la regla condiciónacción para frenar,
 esto es, una función que a partir del estado genera una salida booleana
 (frenar o no frenar).
 En el caso (2), el agente aprende una función que a partir de una imagen
 genera una salida booleana (si la imagen contiene o no un autobús).
 En el caso (3), aprende una función que a partir del estado y las acciones
 para frenar, genera la distancia de parada expresada en pies.
 Nótese que tanto en los casos (1) y (2), un profesor suministra el valor
 correcto de la salida de cada ejemplo; en el tercero, el valor de la salida
 proviene de lo que el agente percibe.
 En entornos totalmente observables, el agente siempre puede observar los
 efectos de sus acciones, y por lo tanto, puede utilizar métodos de aprendizaje
 supervisado para aprender a predecirlos.
 En entornos que son parcialmente observables, el problema es más difícil,
 ya que los efectos más inmediatos pueden ser invisibles.
\end_layout

\begin_layout Subsection
Aprendizaje no supervisado
\end_layout

\begin_layout Standard
El algoritmo busca estructuras en los datos de entrenamiento, como encontrar
 qué ejemplos son similares entre sí, y agruparlos en clusters.
\end_layout

\begin_layout Standard
El problema de aprendizaje no supervisado consiste en aprender a partir
 de patrones de entradas para los que no se especifican los valores de sus
 salidas.
 Por ejemplo, un agente taxista debería desarrollar gradualmente los conceptos
 de «días de tráfico bueno» y de «días de tráfico malo», sin que le hayan
 sido dados ejemplos etiquetados de ello.
 Un agente de aprendizaje supervisado puro no puede aprender qué hacer,
 porque no tiene información de lo que es una acción correcta o un estado
 deseable.
 Principalmente se estudiará aprendizaje no supervisado en el contexto de
 los sistemas de razonamiento probabilístico (Capítulo 20).
\end_layout

\begin_layout Subsection
Aprendizaje por refuerzo
\end_layout

\begin_layout Standard
El problema del aprendizaje por refuerzo, que se describe en el Capítulo
 21, es el más general de las tres categorías.
 En vez de que un profesor indique al agente qué hacer, el agente de aprendizaje
 por refuerzo debe aprender a partir del refuerzo1.
 Por ejemplo, la falta de propina al final del viaje (o una gran factura
 por golpear la parte trasera del coche de delante) da al agente algunas
 indicaciones de que su comportamiento no es el deseable.
 El aprendizaje por refuerzo típicamente incluye el subproblema de aprender
 cómo se comporta el entorno.
 
\end_layout

\begin_layout Standard
La forma de representar la información aprendida, también representa un
 papel importante para determinar el algoritmo de aprendizaje.
 Cualquiera de los componentes de un agente puede ser representado usando
 cualquiera de los esquemas de representación mostrados en este libro.
 Se han mostrado varios ejemplos: polinomios lineales con peso para representar
 funciones de utilidad en programas de teoría de juegos; sentencias en lógica
 proposicional y de primer orden para todas las componentes de un agente
 lógico; y descripciones probabilísticas, como Redes Bayesianas, para las
 componentes de inferencia de un agente basado en la teoría de la decisión.
 Se han ideado algoritmos eficaces de aprendizaje para todos ellos.
 En este capítulo se mostrarán métodos para lógica proposicional, en el
 Capítulo 19 se describen métodos para lógica de primer orden, y en el Capítulo
 20 se tratan métodos para Redes Bayesianas y para redes de neuronas (que
 incluyen polinomios lineales como un caso especial).
 
\end_layout

\begin_layout Standard
El factor más importante en el diseño de sistemas de aprendizaje es la disponibi
lidad de conocimiento a priori.
 La mayoría de los investigadores de aprendizaje en IA, ingeniería informática
 y psicología han estudiado el caso en el que el agente comienza sin información
 sobre lo que está intentando aprender.
 Sólo tiene acceso a los ejemplos de sus experiencias.
 Aunque es un caso importante, no es por término medio el caso más general.
 La mayoría del aprendizaje humano tiene lugar en un contexto con bastante
 conocimiento de base.
 Algunos psicólogos y lingüistas reivindican que incluso los niños recién
 nacidos poseen conocimiento del mundo.
 Verdad o no, no hay duda de que el conocimiento previo puede ayudar en
 gran medida en el aprendizaje.
 Un fí- sico examinando una pila de fotografías de una cámara de burbujas
 (bubble-chamber), podría ser capaz de inferir una teoría que afirmara la
 existencia de una nueva partícula con una cierta masa y una cierta carga;
 sin embargo un crítico de arte que examina la misma pila podría aprender
 únicamente que el «artista» debe pertenecer a alguna clase de impresionismo
 abstracto.
 El Capítulo 19 muestra varios casos en los que se facilita el aprendizaje
 gracias a la existencia de conocimiento; también se muestra cómo el conocimient
o puede ser recopilado para acelerar el proceso de toma de decisiones.
 El Capítulo 20 muestra cómo el conocimiento previo ayuda en el aprendizaje
 de teorías probabilísticas.
\end_layout

\begin_layout Section
Tipos de problemas resueltos por ML
\end_layout

\begin_layout Standard
Teniendo en cuenta las clases de problemas que los algoritmos de aprendizaje
 pueden resolver, los tipos de problemas se pueden agrupar como sigue.
\end_layout

\begin_layout Subsection
Regresión
\end_layout

\begin_layout Standard
Un problema de aprendizaje supervisado donde la respuesta a aprender es
 un valor continuo.
\end_layout

\begin_layout Subsection
Clasificación
\end_layout

\begin_layout Standard
Un problema de aprendizaje supervisado donde la respuesta a aprender es
 un valor de un conjunto finito de posibles valores discretos.
 El problema de estudio se encara como un problema de clasificación, donde
 hay cuatro posibles valores discretos.
\end_layout

\begin_layout Subsection
Segmentación
\end_layout

\begin_layout Standard
Un problema de aprendizaje no supervisado donde la estructura a aprender
 es un conjunto de clusters donde cada cluster tiene similares ejemplos.
\end_layout

\begin_layout Subsection
Análisis de red
\end_layout

\begin_layout Standard
Un problema de aprendizaje no supervisado donde la estructura a aprender
 es información acerca de la importancia y el rol de los nodos en una red.
\end_layout

\begin_layout Section
Problemas de clasificación con ML
\end_layout

\begin_layout Standard
En los problemas de clasificación el modelo creado debe predecir la clase,
 tipo o categoría de la salida.
\end_layout

\begin_layout Subsection
Clasificación binaria
\end_layout

\begin_layout Standard
En su forma más simple se reduce a la siguiente cuestión: dado un patrón
 
\begin_inset Formula $x$
\end_inset

 extraído de un dominio 
\begin_inset Formula $X$
\end_inset

, estimar qué valor asumirá una variable aleatoria binaria asociada 
\begin_inset Formula $y\in\left\{ \pm1\right\} $
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "IntroML2008"

\end_inset

.
\end_layout

\begin_layout Subsection
Clasificación multiclase
\end_layout

\begin_layout Standard
Es la extensión lógica de la clasificación binaria.
 La principal diferencia es que ahora 
\begin_inset Formula $y\in\left\{ 1,2,3..,N\right\} $
\end_inset

 puede asumir un rango de valores diferentes 
\begin_inset CommandInset citation
LatexCommand cite
key "IntroML2008"

\end_inset

.
 El problema de estudio utiliza clasificación multiclase, donde 
\begin_inset Formula $y\in\left\{ Nada,Poco,Medio,Mucho\right\} $
\end_inset


\end_layout

\begin_layout Section
Esquema de machine learning 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset


\end_layout

\begin_layout Standard
Para desarrollar un modelo o esquema de machine learning es necesario conocer
 los componentes esenciales que la forman.
\end_layout

\begin_layout Subsection
Ejemplos o instancias
\end_layout

\begin_layout Standard
La entrada de un esquema de aprendizaje automático es un conjunto de instancias.
 Estas instancias son las cosas que deben ser clasificadas, asociadas o
 agrupadas.
 En el escenario estándar, cada instancia es un ejemplo individual e independien
te del concepto que se debe aprender.
 Para el problema de estudio el proceso de Business Intelligence es quien
 provee las instancias.
\end_layout

\begin_layout Subsection
Características o atributos
\end_layout

\begin_layout Standard
Las instancias son caracterizadas mediante los valores de un conjunto predetermi
nado de atributos.
 Cada instancia proporciona una entrada al aprendizaje automático y es caracteri
zado por los valores de un conjunto fijo y predefinido de características
 o atributos.
\end_layout

\begin_layout Subsection
Etiquetas
\end_layout

\begin_layout Standard
Las cantidades nominales tienen valores que son símbolos distintos.
 Los valores mismos sirven como etiquetas o nombres, de ahí el término nominal,
 que viene de la palabra latina para nombre.
 Los atributos nominales a veces se llaman categorizados, enumerados o discretos.
\end_layout

\begin_layout Subsection
Conjunto de entrenamiento
\end_layout

\begin_layout Standard
El grupo de ejemplos utilizados en el proceso de entrenamiento de los algoritmos
 de aprendizaje automático constituyen el conjunto de entrenamiento.
\end_layout

\begin_layout Subsection
Algoritmos de machine learning
\end_layout

\begin_layout Standard
Hipótesis, Parámetros, Función de costo, Objetivo.
\end_layout

\begin_layout Subsection
Conjunto de prueba
\end_layout

\begin_layout Standard
Para predecir el rendimiento de un clasificador sobre nuevos datos, necesitamos
 evaluar su tasa de error en un conjunto de datos que no desempeñó ningún
 papel en la formación del clasificador.
 Este conjunto de datos independiente se denomina conjunto de prueba.
\end_layout

\begin_layout Section
Algoritmos de clasificación en WEKA
\end_layout

\begin_layout Standard
Weka es una colección de algoritmos de aprendizaje automático para tareas
 de minería de datos.
 Los algoritmos pueden ser aplicados directamente a un conjunto de datos
 o llamados desde código Java.
 Weka contiene herramientas para pre-procesamiento de datos, clasificación,
 regresión, clustering, reglas de asociación y visualización.
 También es adecuado para desarrollar nuevos esquemas de aprendizaje automático
 
\begin_inset CommandInset citation
LatexCommand citet
key "Weka3"

\end_inset

.
 En el problema de estudio se utiliza el conjunto de algoritmos de clasificación
 de Weka 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2016"

\end_inset

.
 Los algoritmos de clasificación de Weka que se utilizarán son los siguientes
 
\begin_inset CommandInset citation
LatexCommand cite
key "WekaCla"

\end_inset

:
\end_layout

\begin_layout Subsection
Bayesianos
\end_layout

\begin_layout Subsubsection
BayesNet
\end_layout

\begin_layout Standard
BayesNet constituye la clase Java base para un clasificador Bayes Network
 (Red Bayesiana).
 Aprende utilizando diversos algoritmos de búsqueda y medidas de calidad.
 Proporciona estructuras de datos (estructura de red, distribuciones de
 probabilidad condicional, etc.) y facilidades comunes a los algoritmos de
 aprendizaje Bayes Network tales como K2 y B 
\begin_inset CommandInset citation
LatexCommand citet
key "WekaMan3-8-0"

\end_inset

.
\end_layout

\begin_layout Subsubsection
NaiveBayes
\end_layout

\begin_layout Standard
NaiveBayes constituye la clase Java base para un clasificador Naive Bayes
 usando estimadores de clases.
 Los valores de precisión del estimador numérico se eligen basándose en
 el análisis de los datos de entrenamiento.
 Por esta razón, el clasificador no es un UpdateableClassifier (que en el
 uso típico se inicializan con cero las instancias de entrenamiento) 
\begin_inset CommandInset citation
LatexCommand citet
key "John1995"

\end_inset

.
\end_layout

\begin_layout Subsubsection
NaiveBayesMultinomial
\end_layout

\begin_layout Standard
NaiveBayesMultinomial constituye la clase Java para construir y utilizar
 un clasificador Naive Bayes multinomial.
 La ecuación central para este clasificador: 
\shape italic

\begin_inset Formula $P\left[C_{i}|D\right]=\left(P\left[D|C_{i}\right]xP\left[C_{i}\right]\right)/P\left[D\right]$
\end_inset


\shape default
 (regla de Bayes), donde 
\shape italic

\begin_inset Formula $C_{i}$
\end_inset


\shape default
 es la clase 
\shape italic

\begin_inset Formula $i$
\end_inset


\shape default
 y 
\shape italic

\begin_inset Formula $D$
\end_inset


\shape default
 es un documento 
\begin_inset CommandInset citation
LatexCommand citet
key "Mccallum1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
NaiveBayesMultinomialUpdateable
\end_layout

\begin_layout Standard
NaiveBayesMultinomial constituye la clase Java para construir y utilizar
 un clasificador Naive Bayes multinomial.
 La ecuación central para este clasificador: 
\shape italic

\begin_inset Formula $P\left[C_{i}|D\right]=\left(P\left[D|C_{i}\right]xP\left[C_{i}\right]\right)/P\left[D\right]$
\end_inset


\shape default
 (regla de Bayes), donde 
\shape italic

\begin_inset Formula $C_{i}$
\end_inset


\shape default
 es la clase 
\shape italic

\begin_inset Formula $i$
\end_inset


\shape default
 y 
\shape italic

\begin_inset Formula $D$
\end_inset


\shape default
 es un documento.
 Es la versión incremental del algoritmo 
\begin_inset CommandInset citation
LatexCommand citet
key "Mccallum1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
NaiveBayesUpdateable
\end_layout

\begin_layout Standard
NaiveBayesUpdateable constituye la clase Java para un clasificador Naive
 Bayes utilizando estimadores de clases.
 Esta es la versión actualizable de NaiveBayes.
 Este clasificador utiliza una precisión predeterminada de 0.1 para atributos
 numéricos cuando se invoca 
\shape italic
buildClassifier
\shape default
 con cero instancias de entrenamiento 
\begin_inset CommandInset citation
LatexCommand citet
key "John1995"

\end_inset

.
\end_layout

\begin_layout Subsection
Basado en funciones
\end_layout

\begin_layout Subsubsection
Logistic
\end_layout

\begin_layout Standard
Logistic constituye la clase Java para construir y utilizar un modelo multinomia
l de regresión logística con un estimador de cresta.
 Si hay 
\shape italic

\begin_inset Formula $k$
\end_inset


\shape default
 clases para 
\shape italic

\begin_inset Formula $n$
\end_inset


\shape default
 instancias con 
\shape italic

\begin_inset Formula $m$
\end_inset


\shape default
 atributos, la matriz de parámetros 
\shape italic

\begin_inset Formula $B$
\end_inset


\shape default
 a calcular será una matriz 
\shape italic

\begin_inset Formula $m*\left(k-1\right)$
\end_inset


\shape default
.
 Aunque la Regresión Logística original no se ocupa de los pesos de las
 instancias, se modifica el algoritmo para manejar los pesos de las instancias
 
\begin_inset CommandInset citation
LatexCommand citet
key "leCessie1992"

\end_inset

.
\end_layout

\begin_layout Subsubsection
MultilayerPerceptron
\end_layout

\begin_layout Standard
MultilayerPerceptron es un clasificador que utiliza 
\shape italic
backpropagation
\shape default
 para clasificar instancias.
 Esta red puede ser monitorizada y modificada durante el tiempo de entrenamiento.
 Los nodos de esta red son todos 
\shape italic
sigmoides
\shape default
 (excepto cuando la clase es numérica, en cuyo caso los nodos de salida
 se convierten en unidades lineales sin umbrales).
\end_layout

\begin_layout Subsubsection
SimpleLogistic
\end_layout

\begin_layout Standard
SimpleLogistic constituye un clasificador para la construcción de modelos
 de regresión logística lineal.
 LogitBoost con funciones de regresión simples como base de aprendizaje
 se utiliza para ajustar los modelos logísticos.
 El número óptimo de iteraciones LogitBoost a realizar es validación cruzada,
 lo que conduce a la selección automática de atributos 
\begin_inset CommandInset citation
LatexCommand citet
key "Landwehr2005"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Sumner2005"

\end_inset

.
\end_layout

\begin_layout Subsubsection
SMO
\end_layout

\begin_layout Standard
Constituye el algoritmo Sequential Minimal Optimization para el entrenamiento
 de un clasificador Support Vector.
 Implementa el algoritmo de optimización mínima secuencial de John Platt.
 Esta implementación reemplaza globalmente todos los valores perdidos y
 transforma los atributos nominales en binarios.
 También normaliza todos los atributos por defecto.
 (En ese caso, los coeficientes de la salida se basan en datos normalizados
 y no en los datos originales).
 Los problemas multiclase se resuelven utilizando la clasificación Pairwise
 (aka 1-vs-1).
 En el caso multiclase, las probabilidades predichas se acoplan utilizando
 el método de acoplamiento Pairwise de Hastie y Tibshirani 
\begin_inset CommandInset citation
LatexCommand citet
key "Platt1998"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Keerthi2001"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Hastie1998"

\end_inset

.
\end_layout

\begin_layout Subsection
Basado en instancias
\end_layout

\begin_layout Subsubsection
IBk
\end_layout

\begin_layout Standard
Clasificador K-nearest neighbours (K vecinos más cercanos).
 Puede seleccionar el valor apropiado de K basado en la validación cruzada.
 También se puede hacer ponderación de distancias 
\begin_inset CommandInset citation
LatexCommand citet
key "Aha1991"

\end_inset

.
\end_layout

\begin_layout Subsubsection
KStar
\end_layout

\begin_layout Standard
K* es un clasificador basado en instancias, es decir, la clase de una instancia
 de prueba se basa en la clase de aquellas instancias de entrenamiento similares
 a ella, según lo determinado por alguna función de similitud.
 Se diferencia de otros esquemas de aprendizaje basados en instancia en
 que utiliza una función de distancia basada en entropía 
\begin_inset CommandInset citation
LatexCommand citet
key "Cleary1995"

\end_inset

.
\end_layout

\begin_layout Subsubsection
LWL
\end_layout

\begin_layout Standard
Locally Weighted Learning (Aprendizaje ponderado localmente) utiliza un
 algoritmo basado en instancias para asignar pesos de instancias que luego
 son utilizados por un especificado 
\shape italic
WeightedInstancesHandler
\shape default
.
 Puede hacer clasificación (por ejemplo, utilizando Naive Bayes) o regresión
 (por ejemplo, utilizando Linear Regression) 
\begin_inset CommandInset citation
LatexCommand citet
key "Frank2003"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Atkeson1996"

\end_inset

.
\end_layout

\begin_layout Subsection
Meta algoritmos
\end_layout

\begin_layout Subsubsection
AdaBoostM1
\end_layout

\begin_layout Standard
AdaBoostM1 constituye la clase Java para impulsar un clasificador de clase
 nominal utilizando el método Adaboost M1.
 Sólo se pueden abordar problemas de clase nominal.
 A menudo mejora dramáticamente el rendimiento, pero a veces sobreajusta
 
\begin_inset CommandInset citation
LatexCommand citet
key "Freund1996"

\end_inset

.
\end_layout

\begin_layout Subsubsection
AttributeSelectedClassifier
\end_layout

\begin_layout Standard
La dimensionalidad de los datos de entrenamiento y de prueba se reduce mediante
 la selección de los atributos antes de pasarlos a un clasificador.
\end_layout

\begin_layout Subsubsection
Bagging
\end_layout

\begin_layout Standard
Bagging constituye la clase Java para capturar un clasificador que reduce
 la varianza.
 Puede hacer clasificación y regresión 
\begin_inset CommandInset citation
LatexCommand citet
key "Breiman1996"

\end_inset

.
\end_layout

\begin_layout Subsubsection
ClassificationViaRegression
\end_layout

\begin_layout Standard
ClassificationViaRegression constituye la clase Java para hacer clasificación
 utilizando métodos de regresión.
 La clase es binarizada y se construye un modelo de regresión por cada valor
 de clase 
\begin_inset CommandInset citation
LatexCommand citet
key "Frank1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
CVParameterSelection
\end_layout

\begin_layout Standard
CVParameterSelection constituye la clase Java para realizar la selección
 de parámetros mediante validación cruzada, para cualquier clasificador
 
\begin_inset CommandInset citation
LatexCommand citet
key "Kohavi1995"

\end_inset

.
\end_layout

\begin_layout Subsubsection
FilteredClassifier
\end_layout

\begin_layout Standard
FilteredClassifier constituye la clase Java para ejecutar un clasificador
 arbitrario en datos que se han pasado a través de un filtro arbitrario.
 Al igual que el clasificador, la estructura del filtro se basa exclusivamente
 en los datos de entrenamiento, y las instancias de prueba serán procesadas
 por el filtro sin cambiar su estructura.
\end_layout

\begin_layout Subsubsection
IterativeClassifierOptimizer
\end_layout

\begin_layout Standard
IterativeClassifierOptimizer elige el mejor número de iteraciones para un
 IterativeClassifier tal como LogitBoost, utilizando validación cruzada.
 Optimiza el número de iteraciones del clasificador iterativo utilizando
 la validación cruzada.
\end_layout

\begin_layout Subsubsection
LogitBoost
\end_layout

\begin_layout Standard
LogitBoost constituye la clase Java para realizar una regresión logística
 aditiva.
 Realiza clasificación utilizando un esquema de regresión como base del
 aprendizaje, y puede manejar problemas multiclase 
\begin_inset CommandInset citation
LatexCommand citet
key "Friedman1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
MultiClassClassifier
\end_layout

\begin_layout Standard
MultiClassClassifier constituye un metaclasificador para manejar conjuntos
 de datos multiclase con clasificadores de 2 clases.
 Este clasificador también es capaz de aplicar códigos de salida de corrección
 de errores para aumentar la precisión.
\end_layout

\begin_layout Subsubsection
MultiClassClassifierUpdateable
\end_layout

\begin_layout Standard
MultiClassClassifierUpdateable constituye un metaclasificador para manejar
 conjuntos de datos multiclase con clasificadores de 2 clases.
 Este clasificador también es capaz de aplicar códigos de salida de corrección
 de errores para aumentar la precisión.
 El clasificador base debe ser un clasificador actualizable.
\end_layout

\begin_layout Subsubsection
MultiScheme
\end_layout

\begin_layout Standard
MultiScheme constituye la clase Java para seleccionar un clasificador entre
 varios, utilizando validación cruzada en los datos de entrenamiento.
 El rendimiento se mide en función del porcentaje de aciertos (clasificación)
 o del error medio cuadrático (regresión).
\end_layout

\begin_layout Subsubsection
RandomCommittee
\end_layout

\begin_layout Standard
RandomCommittee constituye la clase Java para construir un conjunto aleatorizado
 de clasificadores base.
 Cada clasificador base se construye utilizando una semilla de números aleatorio
s diferentes (pero basado en los mismos datos).
 La predicción final es un promedio directo de las predicciones generadas
 por los clasificadores base individuales.
\end_layout

\begin_layout Subsubsection
RandomizableFilteredClassifier
\end_layout

\begin_layout Standard
RandomizableFilteredClassifier constituye la clase Java para ejecutar un
 clasificador arbitrario en datos que han pasado a través de un filtro arbitrari
o.
 Al igual que el clasificador, la estructura del filtro se basa exclusivamente
 en los datos de entrenamiento, y las instancias de prueba serán procesadas
 por el filtro sin cambiar su estructura.
\end_layout

\begin_layout Subsubsection
RandomSubSpace
\end_layout

\begin_layout Standard
Este método construye un clasificador basado en árbol de decisión que mantiene
 la mayor precisión en los datos de entrenamiento y mejora la precisión
 de generalización a medida que crece en complejidad.
 El clasificador consta de múltiples árboles construidos sistemáticamente
 mediante selección pseudoaleatoria de subconjuntos de componentes del vector
 de características, es decir, árboles construidos en subespacios elegidos
 aleatoriamente 
\begin_inset CommandInset citation
LatexCommand citet
key "Ho1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Stacking
\end_layout

\begin_layout Standard
Stacking combina varios clasificadores utilizando el método de apilamiento.
 Puede hacer clasificación o regresión 
\begin_inset CommandInset citation
LatexCommand citet
key "Wolpert1992"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Vote
\end_layout

\begin_layout Standard
Vote constituye la clase Java para combinar clasificadores.
 Se dispone de diferentes combinaciones de estimaciones de probabilidad
 para la clasificación 
\begin_inset CommandInset citation
LatexCommand citet
key "Kuncheva2004"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Kittler1998"

\end_inset

.
\end_layout

\begin_layout Subsubsection
WeightedInstancesHandlerWrapper
\end_layout

\begin_layout Standard
Envoltorio genérico alrededor de cualquier clasificador para permitir soporte
 de instancias ponderadas (weighted instances).
 Utiliza el remuestreo con pesos si el clasificador base no implementa la
 interfaz 
\shape italic
weka.core.WeightedInstancesHandler
\shape default
 y hay otros pesos de instancias 1.0 presentes.
 De forma predeterminada, los datos de entrenamiento se pasan al clasificador
 base si puede manejar pesos de instancia.
 Sin embargo, es posible forzar el uso del remuestreo con pesos también.
\end_layout

\begin_layout Subsection
Sistema de reglas
\end_layout

\begin_layout Subsubsection
DecisionTable
\end_layout

\begin_layout Standard
Constituye la clase Java para la construcción y uso de un clasificador de
 mayoría de una tabla de decisión simple 
\begin_inset CommandInset citation
LatexCommand citet
key "Kohavi1995a"

\end_inset

.
\end_layout

\begin_layout Subsubsection
JRip
\end_layout

\begin_layout Standard
Implementa el aprendizaje de reglas proposicionales 
\begin_inset Quotes eld
\end_inset

Repeated Incremental Pruning to Produce Error Reduction
\begin_inset Quotes erd
\end_inset

 (RIPPER) o 
\begin_inset Quotes eld
\end_inset

Poda Incremental Repetida
\begin_inset Quotes erd
\end_inset

 para producir reducción de errores.
 Fue propuesto por William W.
 Cohen como una versión optimizada de IREP 
\begin_inset CommandInset citation
LatexCommand citet
key "Cohen1995"

\end_inset

.
\end_layout

\begin_layout Subsubsection
OneR
\end_layout

\begin_layout Standard
Constituye la clase Java para construir y utilizar un clasificador 1R.
 Utiliza el atributo de error mínimo para la predicción, discretizando los
 atributos numéricos 
\begin_inset CommandInset citation
LatexCommand citet
key "Holte1993"

\end_inset

.
\end_layout

\begin_layout Subsubsection
PART
\end_layout

\begin_layout Standard
Utiliza dividir y conquistar.
 Construye un árbol de decisión C4.5 parcial.
 Constituye la clase Java para generar una lista de decisiones PART.
 Crea un árbol de decisión C4.5 parcial en cada iteración y convierte la
 "mejor" hoja en una regla 
\begin_inset CommandInset citation
LatexCommand citet
key "Frank1998a"

\end_inset

.
\end_layout

\begin_layout Subsubsection
ZeroR
\end_layout

\begin_layout Standard
Constituye la clase Java para construir y usar un clasificador 0-R.
 Predice la media (para una clase numérica) o la moda (para una clase nominal).
\end_layout

\begin_layout Subsection
Árboles de decisión
\end_layout

\begin_layout Subsubsection
DecisionStump
\end_layout

\begin_layout Standard
Constituye la clase Java para construir y utilizar un tocón de decisión.
 Generalmente se utiliza en conjunción con un algoritmo de boosting.
 Realiza regresión (basado en el error cuadrático medio) o clasificación
 (basado en la entropía).
\end_layout

\begin_layout Subsubsection
HoeffdingTree
\end_layout

\begin_layout Standard
Un árbol Hoeffding (VFDT) es un algoritmo de inducción de árbol de decisión
 incremental que es capaz de aprender de flujos de datos masivos, suponiendo
 que la distribución de la generación de los ejemplos no cambian con el
 tiempo.
 Los árboles Hoeffding explotan el hecho de que una pequeña muestra puede
 a menudo ser suficiente para elegir un atributo de división óptimo.
 Esta idea está apoyada matemáticamente por el límite de Hoeffding, que
 cuantifica el número de observaciones (en nuestro caso, ejemplos) necesarios
 para estimar algunas estadísticas dentro de una precisión prescrita (en
 nuestro caso, la bondad de un atributo).
 Una característica teóricamente atractiva de Hoeffding Trees no compartida
 por otros aprendizajes por árboles de decisión incremental es que tiene
 garantías sólidas de rendimiento.
 Utilizando el límite de Hoeffding se puede demostrar que su salida es asintótic
amente casi idéntica a la de un aprendizaje no incremental usando infinidad
 de ejemplos 
\begin_inset CommandInset citation
LatexCommand citet
key "Hulten2001"

\end_inset

.
\end_layout

\begin_layout Subsubsection
J48
\end_layout

\begin_layout Standard
Constituye la clase Java para generar un árbol de decisión C4.5 podado o
 no podado 
\begin_inset CommandInset citation
LatexCommand citet
key "Quinlan1993"

\end_inset

.
\end_layout

\begin_layout Subsubsection
LMT
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Árboles de Modelos Logísticos
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

Logistic Model Trees
\begin_inset Quotes erd
\end_inset

 (LMT).
 Clasificador para la construcción de árboles de modelos logísticos, que
 son árboles de clasificación con funciones de regresión logística en las
 hojas.
 El algoritmo puede manejar variables binarias y multiclases, atributos
 numéricos y nominales y valores faltantes 
\begin_inset CommandInset citation
LatexCommand citet
key "Landwehr2005"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Sumner2005"

\end_inset

.
\end_layout

\begin_layout Subsubsection
RandomForest
\end_layout

\begin_layout Standard
Constituye la clase Java para construir un 
\begin_inset Quotes eld
\end_inset

Bosque de Árboles Aleatorios
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

Forest of Random Trees
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Breiman2001"

\end_inset

.
\end_layout

\begin_layout Subsubsection
RandomTree
\end_layout

\begin_layout Standard
Constituye la clase Java para construir un árbol que considera K atributos
 elegidos al azar en cada nodo.
 No realiza poda.
 También tiene una opción que permite la estimación de probabilidades de
 clase (o media objetivo en el caso de regresión) basado en un conjunto
 de retención (backfitting).
\end_layout

\begin_layout Subsubsection
REPTree
\end_layout

\begin_layout Standard
Aprendizaje rápido con árboles de decisión.
 Construye un árbol de decisión/regresión utilizando la información de ganancia/
varianza y la elimina utilizando poda de reducción de errores (con backfitting-a
juste posterior).
 Sólo ordena valores para atributos numéricos.
 Los valores faltantes se tratan dividiendo las instancias correspondientes
 en fragmentos (es decir, como en C4.5).
\end_layout

\begin_layout Section
Evaluación del aprendizaje
\end_layout

\begin_layout Standard
La evaluación es la clave para lograr avances reales en el aprendizaje automátic
o.
 Entre las técnicas de evaluación se destacan la Validación Cruzada (Cross-Valid
ation) y la Validación Cruzada k-pliegues Estratificado (Stratified k-fold
 Cross-Validation).
 
\end_layout

\begin_layout Standard
La técnica de Cross-Validation consiste en dividir los datos en un número
 de pliegues o particiones, si por ejemplo elegimos cuatro, entonces cada
 partición se utiliza para las pruebas y las demás para el entrenamiento,
 al repetir este proceso 4 veces se consigue que cada partición se haya
 utilizado una vez como conjunto de pruebas.
 
\end_layout

\begin_layout Standard
La técnica estándar para predecir la tasa de error es Stratified k-fold
 Cross-Validation, donde la estratificación se refiere al proceso de reorganizar
 los datos de tal manera a asegurar que cada pliegue sea una buena representació
n del conjunto.
 Comúnmente se acepta que 10 es el número de pliegues con el que se obtiene
 la mejor estimación de error, idea basada en diversas pruebas sobre conjuntos
 de datos diferentes y para distintas técnicas de aprendizaje 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset

.
\end_layout

\begin_layout Standard
Otra técnica es el Porcentaje de División (Percentage Split) con el que
 puede retener para la prueba un determinado porcentaje de los datos.
 Es una alternativa utilizar un conjunto de pruebas separado o una división
 porcentual de los datos de entrenamiento.
 Si elegimos 60% como porcentaje de división, entonces el conjunto de prueba
 se constituirá con el 40% de las instancias y el conjunto de entrenamiento
 con el 60% de las instancias.
\end_layout

\begin_layout Section
Métricas de desempeño 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset


\end_layout

\begin_layout Standard
Para los problemas de clasificación, es natural medir el rendimiento de
 un clasificador en términos de la tasa de error (error rate).
 El clasificador predice la clase de cada instancia: si es correcta se cuenta
 como un éxito, sino se cuenta como un error.
 La tasa de error es sólo la proporción de errores cometidos sobre un conjunto
 de instancias, y mide el rendimiento general del clasificador.
 Por supuesto, lo que nos interesa es el probable desempeño futuro en nuevos
 datos, no el rendimiento pasado en datos antiguos.
\end_layout

\begin_layout Standard
Para predecir el rendimiento de un clasificador en nuevos datos, necesitamos
 evaluar su tasa de error en un conjunto de datos que no desempeñó ningún
 papel en la formación del clasificador.
 Este conjunto de datos independiente se denomina conjunto de prueba.
 En tales situaciones se suele hablar de tres conjuntos de datos: los datos
 de entrenamiento, los datos de validación y los datos de prueba.
 
\end_layout

\begin_layout Standard
Los datos de entrenamiento son utilizados por uno o más esquemas de aprendizaje
 para conocer clasificadores.
 Los datos de validación se utilizan para optimizar los parámetros de los
 clasificadores, o para seleccionar uno determinado.
 A continuación, los datos de prueba se utilizan para calcular la tasa de
 error del método final optimizado.
 Cada uno de los tres conjuntos debe ser independiente: El conjunto de validació
n debe ser diferente del conjunto de entrenamiento para obtener un buen
 desempeño en la etapa de optimización o selección y el conjunto de pruebas
 debe ser diferente de ambos para obtener una estimación confiable de la
 tasa de error real.
\end_layout

\begin_layout Subsection
Aciertos
\end_layout

\begin_layout Standard
Número de instancias correctamente clasificadas.
\end_layout

\begin_layout Subsection
Porcentaje de Aciertos
\end_layout

\begin_layout Standard
Porcentaje de instancias correctamente clasificadas.
\end_layout

\begin_layout Subsection
Estadística Kappa (Kappa Statistic)
\end_layout

\begin_layout Standard
En problemas de clasificación para aplicaciones reales normalmente los errores
 cuestan diferentes cantidades.
 Por ejemplo en bancos y financieras el costo de prestar a una persona que
 no paga sus deudas es mayor que el costo de rechazar un préstamo a una
 persona que es pagadora.
 Los Verdaderos Positivos (True Positive - TP) y Verdaderos Negativos (True
 Negative - TN) son clasificaciones correctas.
 Un Falso Positivo (False Positive - FP) es cuando el resultado se predice
 incorrectamente como sí (o positivo) cuando es realmente no (o negativo).
 Un Falso Negativo (False Negative - FN) es cuando el resultado se predice
 incorrectamente como negativo cuando es realmente positivo.
 En la predicción multiclase, cada elemento de la matriz de confusión muestra
 el número de ejemplos de prueba para los que la clase real es la fila y
 la clase prevista es la columna.
 Son buenos resultados los grandes números en la diagonal principal e idealmente
 cero fuera de la diagonal principal.
 
\begin_inset Quotes eld
\end_inset

Kappa se utiliza para medir el acuerdo entre la predicción y la observación
 de las categorizaciones de un conjunto de datos, mientras que se corrige
 para un acuerdo que ocurre por casualidad
\begin_inset Quotes erd
\end_inset

.
 Si los evaluadores están totalmente de acuerdo Kappa alcanza un valor máximo
 igual a 1.
 Si no hay total acuerdo entre los evaluadores, entonces Kappa tiene un
 valor 
\begin_inset Formula $<1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\kappa=\frac{Pr\left(a\right)-Pr\left(e\right)}{1-Pr\left(e\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Donde: 
\begin_inset Formula $Pr\left(a\right)$
\end_inset

 es el acuerdo observado relativo entre los observadores y 
\begin_inset Formula $Pr\left(e\right)$
\end_inset

 es la probabilidad hipotética de acuerdo al azar utilizando los datos observado
s para calcular las probabilidades de que cada observador clasifique aleatoriame
nte cada categoría.
\end_layout

\begin_layout Subsection
Sensibilidad (Recall)
\end_layout

\begin_layout Standard
Calcula la sensibilidad con respecto a una clase en particular, esto se
 define como: positivos correctamente clasificados / positivos totales.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Recall=\frac{TP}{TP+FN}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Precisión (Precision)
\end_layout

\begin_layout Standard
Calcula la precisión con respecto a una clase en particular, esto se define
 como: positivos correctamente clasificados / total predicho como positivo.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Precision=\frac{TP}{TP+FP}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Puntuación-F (F-Measure)
\end_layout

\begin_layout Standard
La Puntuación-F es una medida de la exactitud de una prueba.
 La Puntuación-F puede interpretarse como un promedio ponderado de la precisión
 y sensibilidad, donde alcanza su mejor valor en 1 y el peor en 0.
 Se define como: 2 * Recall * Precision / (Recall + Precision).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
F-Measure=\frac{2*Recall*Precision}{\left(Recall+Precision\right)}
\end{equation}

\end_inset


\end_layout

\end_body
\end_document
