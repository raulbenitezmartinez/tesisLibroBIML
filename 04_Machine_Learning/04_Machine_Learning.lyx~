#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\renewcommand{\listtablename}{Indice de tablas}
\renewcommand{\tablename}{Tabla} 
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 1.5cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Machine Learning
\end_layout

\begin_layout Standard
El objetivo de este capítulo es visualizar qué aspectos de Machine Learning
 fueron tomados como componentes de solución al problema de estudio.
\end_layout

\begin_layout Standard
En lo que va de la Edad Contemporánea, el hombre siente fuertemente la necesidad
 de encontrar respuestas a ciertos aspectos de sus propias capacidades.
 Respuestas de cómo funciona el cerebro humano, cómo se van originando e
 hilando los pensamientos, cómo se va adquiriendo el conocimiento, cómo
 la racionalidad está presente en las decisiones humanas, cuál es el mecanismo
 de toma de decisiones de la mente humana, cómo utiliza su inteligencia
 para resolver problemas e ideas abstractas, cuál es el proceso que sigue
 la mente para poder aprender, interrogantes sobre la memoria del cerebro
 y cómo los sentidos van alimentando de percepciones para tener una visión
 única del universo.
 La ciencia de la computación ha abierto la puerta a dichas respuestas.
\end_layout

\begin_layout Standard
En 1950 Alan Turing dio uno de los saltos más importantes al proponer el
 enfoque de su 
\begin_inset Quotes eld
\end_inset

Prueba de Turing
\begin_inset Quotes erd
\end_inset

, donde un computador es considerado como agente inteligente si un evaluador
 humano, sin interactuar con el computador, realiza preguntas y no es capaz
 de distinguir si las respuestas vienen de una persona o del computador.
 En su artículo 
\begin_inset Quotes eld
\end_inset

Computing Machinery and Intelligence
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "CMIAlan1950"

\end_inset

 expuso este y otros conceptos y como tal es considerado padre de la Inteligenci
a Artificial (IA
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IA"
description "Inteligencia Artificial"

\end_inset

).
\end_layout

\begin_layout Standard
Aunque hoy en día aún es válida la Prueba de Turing, el desafío no está
 en construir un agente inteligente total, sino mas bien buscar que un computado
r tenga capacidades como 
\begin_inset CommandInset citation
LatexCommand cite
key "IAEModerno2"

\end_inset

:
\end_layout

\begin_layout Enumerate
Procesamiento del lenguaje natural
\end_layout

\begin_layout Enumerate
Representación del conocimiento
\end_layout

\begin_layout Enumerate
Razonamiento automático
\end_layout

\begin_layout Enumerate
Aprendizaje automático (Machine Learning)
\end_layout

\begin_layout Enumerate
Visión computacional
\end_layout

\begin_layout Enumerate
Robótica
\end_layout

\begin_layout Standard
Entre los precursores de IA están John McCarthy, Marvin Minsky, Allen Newell
 y Herbert Simon, todos ganadores del premio 
\begin_inset Quotes eld
\end_inset

ACM A.M.
 Turing Award
\begin_inset Quotes erd
\end_inset

 por sus notables aportes en los inicios de IA.
 McCarthy acuño el término Artificial Intelligence en el taller 
\begin_inset Quotes eld
\end_inset

Dartmouth Summer Research Project on Artificial Intelligence
\begin_inset Quotes erd
\end_inset

 que organizó y se desarrolló en junio del año 1956.
 En palabras transcriptas, el taller perseguía el siguiente objetivo: 
\shape italic

\begin_inset Quotes eld
\end_inset

El estudio debe proceder sobre la base de la conjetura de que cada aspecto
 del aprendizaje o cualquier otra característica de la inteligencia puede,
 en principio, describirse tan precisamente que se puede hacer que una máquina
 lo simule.
 Se intentará encontrar cómo hacer que las máquinas usen el lenguaje, formen
 abstracciones y conceptos, resuelvan tipos de problemas ahora reservados
 para los humanos y se mejoren a sí mismos.
 Creemos que se puede lograr un avance significativo en uno o más de estos
 problemas si un grupo de científicos cuidadosamente seleccionados trabajan
 juntos durante un verano
\begin_inset Quotes erd
\end_inset


\shape default
.
 Entre los aportes mas destacados de McCarthy se pueden mencionar la definición
 del lenguaje de alto nivel Lisp, la creación del tiempo compartido y el
 artículo 
\begin_inset Quotes eld
\end_inset

Programs with Common Sense
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "PCSMc1959"

\end_inset

 donde define su 
\shape italic
Generador de Consejos
\shape default
, todos estos aportes desarrollados en 1958.
 Minsky diseñó SNARC
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SNARC"
description "Stochastic Neural Analog Reinforcement Calculator"

\end_inset

 (Stochastic Neural Analog Reinforcement Calculator) la primera máquina
 a partir de una red neuronal, en el año 1951, junto a Seymour Papert publicó
 el libro 
\begin_inset Quotes eld
\end_inset

Perceptrons: an introduction to computational geometry
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "PercepMinsPape1987"

\end_inset

 en el año 1969 y también creó un modelo de redes semánticas denominadas
 
\begin_inset Quotes eld
\end_inset

marcos
\begin_inset Quotes erd
\end_inset

 publicado en 
\begin_inset Quotes eld
\end_inset

A framework for representing knowledge
\begin_inset Quotes erd
\end_inset

 en el año 1974 
\begin_inset CommandInset citation
LatexCommand cite
key "FrameMinsky1974"

\end_inset

.
\end_layout

\begin_layout Standard
En setiembre de 1956 en el 
\begin_inset Quotes eld
\end_inset

MIT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "MIT"
description "Massachusetts Institute of Technology"

\end_inset

 Symposium on Information Theory
\begin_inset Quotes erd
\end_inset

 se mostraron trabajos muy importantes como el de George Miller que presentó
 
\begin_inset Quotes eld
\end_inset

The Magical Number Seven, Plus or Minus Two: Some Limits on our Capacity
 for Processing Information
\begin_inset Quotes erd
\end_inset

, Noam Chomsky presentó 
\begin_inset Quotes eld
\end_inset

Three Models for the Description of Language
\begin_inset Quotes erd
\end_inset

, y Allen Newell y Herbert Simon presentaron 
\begin_inset Quotes eld
\end_inset

The Logic Theory Machine
\begin_inset Quotes erd
\end_inset

: programa de computador capaz de hacer la demostración de un teorema y
 considerado el primer programa de IA.
 El campo de la ciencia cognitiva echó sus raíces en este simposio evidenciando
 que los modelos informáticos se pueden utilizar para modelar la psicología
 de la memoria, el lenguaje y el pensamiento lógico.
 En 1959 presentaron el programa 
\begin_inset Quotes eld
\end_inset

General Problem Solver
\begin_inset Quotes erd
\end_inset

 (GPS
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "GPS"
description "General Problem Solver"

\end_inset

), pretendía funcionar como una máquina universal para resolver problemas
 y fue el primer programa de computador que separó su conocimiento de los
 problemas (reglas representadas como datos de entrada) de su estrategia
 de cómo resolver problemas (un motor de resolución genérico).
\end_layout

\begin_layout Standard
Machine Learning como subcampo de IA está presente en muchas aplicaciones
 de la vida real, especialmente en aquellas donde se requiere el procesamiento
 de grandes cantidades de datos.
 Por esta razón la tecnología de la información la toma como aliado esencial.
 Muchos avances tecnológicos de última generación utilizan algoritmos de
 Machine Learning para realizar un análisis inteligente de los datos 
\begin_inset CommandInset citation
LatexCommand cite
key "IntroML2008"

\end_inset

.
 Entre algunas aplicaciones conocidas se destacan:
\end_layout

\begin_layout Itemize
Reconocimiento facial de Facebook: técnicas de Computer Vision and Pattern
 Recognition.
 https://research.fb.com/learning-to-segment/
\end_layout

\begin_layout Itemize
Kinect para Xbox 360: técnicas de reconocimiento de voz y reconocimiento
 facial para la identificación automática de los usuarios.
\end_layout

\begin_layout Itemize
Voice reconigtion.
\end_layout

\begin_layout Itemize
La tecnología del habla y el campo relacionado del reconocimiento de caracteres
 manuscritos.
\end_layout

\begin_layout Itemize
Motores de búsqueda, sistemas de recomendación, y los sistemas para la construcc
ión de portales Web.
\end_layout

\begin_layout Itemize
Recomender system en plataformas como Amazon, Netflix, Facebook .
\end_layout

\begin_layout Itemize
Reconocimiento automático de ciertas áreas en el mundo realizado por satélites.
\end_layout

\begin_layout Standard
Para recalcar el veloz crecimiento de los datos en el mundo, en un estudio
 publicado por la International Data Corporation (IDC
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IDC"
description "International Data Corporation"

\end_inset

) y patrocinado por DELL EMC que se denomina: 
\begin_inset Quotes eld
\end_inset

The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest
 Growth in the Far East
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "DUGrow2012"

\end_inset

 se analiza qué tan rápido crecen los datos cada año, una medida que incluyen
 todos los datos digitales creados, replicados y consumidos en un solo año.
 En la Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:IDC-exabyes"

\end_inset

 se muestra una proyección del tamaño hasta el 2020.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/THE DIGITAL UNIVERSE IN 2020.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:IDC-exabyes"

\end_inset

Proyección de crecimiento de datos del 2005 al 2020.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
El universo digital analizado está compuesto por imágenes y vídeos en teléfonos
 móviles cargados en YouTube, películas digitales, datos bancarios en cajeros
 automáticos, imágenes de seguridad en aeropuertos y en eventos importantes
 como los Juegos Olímpicos, colisiones subatómicos registradas por el Gran
 Colisionador de Hadrones en el CERN
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "CERN"
description "Conseil Europeen pour la Recherche Nucleaire"

\end_inset

, llamadas de voz a través de líneas telefónicas digitales y los mensajes
 de texto.
 Las mediciones indican que en el 2005 existían 130 exabytes en datos digitales,
 en el 2010 llegó a 1200 exabytes, en el 2015 a unos 7900 exabytes y para
 el 2020 se pronostica que llegará a los 40000 exabytes.
 IDC estima que para 2020, hasta el 33% del universo digital contendrá informaci
ón que podría ser valiosa si se analiza.
\end_layout

\begin_layout Section
Definición
\end_layout

\begin_layout Standard
Arthur Samuel acuñó el término Machine Learning en 1959 mientras trabaja
 para IBM
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IBM"
description "International Business Machines"

\end_inset

.
 En el IBM Journal publicó 
\begin_inset Quotes eld
\end_inset

Some Studies in Machine Learning Using the Game of Checker
\begin_inset Quotes erd
\end_inset

 en el cual escribió: 
\shape italic

\begin_inset Quotes eld
\end_inset

Programming computers to learn from experience should eventually eliminate
 the need for much of this detailed programming effort
\begin_inset Quotes erd
\end_inset


\shape default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Arthur1959"

\end_inset

 por lo que de su artículo se interpreta que 
\shape italic

\begin_inset Quotes eld
\end_inset

Machine Learning es un campo de estudio que da a las computadoras la capacidad
 de aprender a resolver problemas sin ser explícitamente programados
\shape default

\begin_inset Quotes erd
\end_inset

.
 Samuel desarrolló en el año 1952 un programa que aprendió a jugar Damas,
 hasta llegar a una categoría equivalente al amateur.
 Este pionero ya demostraba que los programas a través del aprendizaje pueden
 efectuar tareas de toma de decisiones sin ser programadas explícitamente
 dichas decisiones.
 Cabe destacar que Arthur Samuel fue uno de los asistentes del 
\begin_inset Quotes eld
\end_inset

Dartmouth Summer Research Project on Artificial Intelligence
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Otro investigador, Tom Mitchell propuso en 1998 la siguiente definición:
 
\shape italic

\begin_inset Quotes eld
\end_inset

Well posed Learning Problem: A computer program is said to learn from experience
 E with respect to some task T and some performance measure P, if its performanc
e on T, as measured by P, improves with experience E
\begin_inset Quotes erd
\end_inset


\shape default
.
 Donde se nos indica que el aprendizaje en las máquinas deberá ser parecido
 al aprendizaje en los humanos, por ejemplo cuando una criatura comienza
 a hablar a través de la experiencia de pronunciar las palabras y de su
 interacción con otras personas, entonces sucede que su capacidad de hablar
 se va perfeccionando o mejorando.
\end_layout

\begin_layout Standard
Otra definición es 
\shape italic

\begin_inset Quotes eld
\end_inset

The purpose of machine learning is to learn from training data in order
 to make as good as possible predictions on new, unseen, data
\begin_inset Quotes erd
\end_inset


\shape default

\begin_inset CommandInset citation
LatexCommand cite
key "Jean2016"

\end_inset

.
 La dificultad radica en que debemos construir modelos que nos acerquen
 a una buena predicción sobre datos aún no conocidos o imprevistos.
\end_layout

\begin_layout Standard
En el contexto de IA y según el enfoque propuesto en el libro 
\begin_inset Quotes eld
\end_inset

Inteligencia Artificial Un Enfoque Moderno
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "IAEModerno2"

\end_inset

, se conciben los sistemas inteligentes como agentes racionales dotados
 de capacidades específicas.
 Entre las capacidades principales están el poder percibir el entorno de
 trabajo (
\begin_inset Quotes eld
\end_inset

problemas para los cuales fueron hechos
\begin_inset Quotes erd
\end_inset

) con la ayuda de sus sensores, actuar en ese entorno mediante sus actuadores
 y contar con una medida de rendimiento para medir el éxito.
 Estructuralmente un agente = arquitectura + programa.
 Los sensores y actuadores forman parte de la arquitectura, la arquitectura
 es el medio físico que podría ser una PC
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "PC"
description "Personal Computer"

\end_inset

 común o un sofisticado robot con brazos mecánicos, cámaras, sensores, etc.
 El objetivo principal de IA es construir el programa del agente, que es
 donde se implementa la función del agente.
 Para que el agente sea considerado un agente que aprende, el programa debe
 contener el 
\begin_inset Quotes eld
\end_inset

Elemento de aprendizaje
\begin_inset Quotes erd
\end_inset

.
 En la Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Agente-racional"

\end_inset

 se grafica el concepto, un modelo de agente inteligente que aprende.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/Agentes que aprenden.png
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Agente-racional"

\end_inset

Modelo general para agentes que aprenden 
\begin_inset CommandInset citation
LatexCommand cite
key "IAEModerno2"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Formas de Aprendizaje 
\end_layout

\begin_layout Standard
Los algoritmos de aprendizaje automático se pueden agrupar según la forma
 en que se realiza el aprendizaje, conforme a la información que poseen
 o que pueden llegar a poseer.
 Uno de los componentes más importantes al momento de diagnosticar la naturaleza
 del problema de aprendizaje es el tipo de retroalimentación disponible
 para el aprendizaje.
 Hay tres tipos distintos de aprendizaje: supervisado, no supervisado y
 por refuerzo 
\begin_inset CommandInset citation
LatexCommand citet
key "IAEModerno2"

\end_inset

.
\end_layout

\begin_layout Subsection
Aprendizaje supervisado
\end_layout

\begin_layout Standard
Los agentes inteligentes que implementan algoritmos de apredizaje supervisado
 (
\shape italic
Supervised Learning
\shape default
), tienen como objetivo aprender una función de hipótesis 
\begin_inset Formula $h$
\end_inset

 que se aproxime a la función verdadera 
\begin_inset Formula $f$
\end_inset

 y que es solución al problema que se intenta resolver.
 Lo que se conoce de 
\begin_inset Formula $f$
\end_inset

 son los llamados 
\begin_inset Quotes eld
\end_inset

ejemplos
\begin_inset Quotes erd
\end_inset

, que son puntos concretos dentro de la desconocida linea que representa
 la función verdadera 
\begin_inset Formula $f$
\end_inset

.
 En otras palabras, la definición matemática de la función 
\begin_inset Formula $f$
\end_inset

 no se conoce, solamente se conoce un conjunto de ejemplos definidos por
 valores correctos de entradas y salidas de la función 
\begin_inset Formula $f$
\end_inset

 .
\end_layout

\begin_layout Standard
Otra forma de verlo es de la siguiente manera, se dispone de variables de
 entrada 
\begin_inset Formula $(X)$
\end_inset

 y una variable de salida 
\begin_inset Formula $(Y)$
\end_inset

, mediante un algoritmo y a partir de un conjunto de datos de entrenamiento
 se busca aprender una función 
\begin_inset Formula $Y=f(X)$
\end_inset

 que mapee la salida desde la entrada.
 Los datos de entrenamiento constituyen las respuestas correctas conocidas
 o 
\begin_inset Quotes eld
\end_inset

ejemplos
\begin_inset Quotes erd
\end_inset

.
 El objetivo esencial es aproximar la función de mapeo lo mejor que se pueda,
 de manera que cuando se tengan nuevos datos de entrada 
\begin_inset Formula $(X)$
\end_inset

 se pueda predecir la variable de salida 
\begin_inset Formula $(Y)$
\end_inset

.
\end_layout

\begin_layout Standard
Problemas del tipo clasificación o regresión se resuelven normalmente con
 aprendizaje supervisado.
 Un problema de clasificación es cuando la variable de salida es una categoría
 y un problema de regresión es cuando la variable de salida es un valor
 real.
 Como ejemplo de aplicaciones que implementan esta técnica están los vehículos
 autoconducidos, que deben aprender a diferenciar una calle y de la que
 no es (salida booleana: es calle o no es calle?), también debe aprender
 a frenar (salida booleana: frenar o no frenar?), etc.
 Entre los algoritmos que implementan aprendizaje supervisado están: Logistic
 Regression, Linear Regression, Support Vector Machines (SVM
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SVM"
description "Support Vector Machines"

\end_inset

), Back Propagation Neural Network, etc.
 El problema de estudio de la tesis utiliza algoritmos de aprendizaje supervisad
o, donde el experto en compras da la respuesta correcta a cada ejemplo.
\end_layout

\begin_layout Subsection
Aprendizaje no supervisado
\end_layout

\begin_layout Standard
En los problemas de aprendizaje no supervisado (
\shape italic
Unsupervised Learning
\shape default
) los algoritmos reciben como entrada datos de entrenamiento (ejemplos)
 que no tienen respuestas correctas conocidas.
 Solo se dispone de variables de entrada 
\begin_inset Formula $(X)$
\end_inset

 pero no hay correspondiente variable de salida 
\begin_inset Formula $(Y)$
\end_inset

.
 Los algoritmos buscan estructuras presentes y como resultado pueden extraer
 reglas generales, o reducir sistemáticamente la redundancia, o se pueden
 organizar los datos por similitud.
 Los problemas de agrupamiento (clustering) y asociación (association) se
 resuelven normalmente con algoritmos de aprendizaje no supervisados.
\end_layout

\begin_layout Standard
Como ejemplo de implementaciones está el caso de la computadora que aprendió
 sola el concepto de lo que es un animal gato, otro ejemplo es el reconocimiento
 de dígitos escritos a mano.
 Entre los algoritmos que implementan aprendizaje no supervisado están:
 k-Means, Principal Component Analysis (PCA
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "PCA"
description "Principal Component Analysis"

\end_inset

), Hierarchical Clustering.
\end_layout

\begin_layout Subsection
Aprendizaje por refuerzo
\end_layout

\begin_layout Standard
El aprendizaje por refuerzo (
\shape italic
Reinforcement Learning
\shape default
) es el más general entre las tres categorías.
 En vez de que un instructor indique al agente qué hacer, el agente inteligente
 debe aprender cómo se comporta el entorno mediante recompensas (
\shape italic
refuerzos
\shape default
) o castigos, derivados del éxito o del fracaso respectivamente.
 El objetivo principal es aprender la función de valor que le ayude al agente
 inteligente a maximizar la señal de recompensa y así optimizar sus políticas
 de modo a comprender el comportamiento del entorno y a tomar buenas decisiones
 para el logro de sus objetivos formales.
\end_layout

\begin_layout Standard
Los principales algoritmos de aprendizaje por refuerzo se desarrollan dentro
 de los métodos de resolución de problemas de decisión finitos de Markov,
 que incorporan las ecuaciones de Bellman y las funciones de valor.
 Los tres métodos principales son: la Programación Dinámica (
\shape italic
Dynamic Programming
\shape default
 
\shape italic
o DP
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "DP"
description "Dynamic Programming"

\end_inset

), los métodos de Monte Carlo y el aprendizaje de Diferencias Temporales
 (
\shape italic
Temporal-Difference Learning o TD
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "TD"
description "Temporal-Difference Learning"

\end_inset

) 
\begin_inset CommandInset citation
LatexCommand cite
key "RLIntroduccion1998"

\end_inset

.
\end_layout

\begin_layout Standard
Entre las implementaciones desarrolladas está AlphaGo, un programa de IA
 desarrollado por Google DeepMind para jugar el juego de mesa Go.
 En marzo de 2016 AlphaGo le ganó una partida al jugador profesional Lee
 Se-Dol que tiene la categoría noveno dan y 18 títulos mundiales.
 Entre los algoritmos que utiliza se encuentra el árbol de búsqueda Monte
 Carlo, también utiliza aprendizaje profundo con redes neuronales.
 Puede ver lo ocurrido en el documental de Netflix 
\begin_inset Quotes eld
\end_inset

AlphaGo
\begin_inset Quotes erd
\end_inset

 https://www.netflix.com/title/80190844.
\end_layout

\begin_layout Section
Algoritmos de aprendizaje supervisado y no supervisado
\end_layout

\begin_layout Standard
El Dr.
 Jason Brownlee es un especialista en aprendizaje automático, desarrollador,
 escritor y empresario.
 Ha trabajado en sistemas de aprendizaje automático para la defensa, startups
 y pronósticos meteorológicos.
 Tiene una comunidad en https://machinelearningmastery.com/, la cual empezó
 porque le apasiona ayudar a los desarrolladores profesionales a comenzar
 y aplicar con confianza Machine Learning que les permita resolver problemas
 complejos.
 Los algoritmos de aprendizaje automático se pueden agrupar según la similaridad
 en términos de su forma o función, como por ejemplo los métodos basados
 en árboles y los métodos inspirados en redes neuronales.
 Se muestra en la Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Algoritmos-de-machine"

\end_inset

 lo propuesto por el Dr.
 Jason:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/MachineLearningAlgorithms.png
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Algoritmos-de-machine"

\end_inset

Agrupación de algoritmos según el Dr.
 Jason.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
No hay un consenso general de cómo agrupar los algoritmos de Machine Learning
 en términos de su función o de cómo trabajan.
 La Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Algoritmos-de-machine"

\end_inset

 mostró un método útil de agrupación, que no es perfecto y ni exhaustivo
 en los grupos y algoritmos.
 Hay algoritmos que pueden encajar en varias categorías como Learning Vector
 Quantization que es a la vez un método inspirado en una red neuronal y
 un método basado en instancia.
 También hay categorías que tienen el mismo nombre que describen el problema
 y la clase de algoritmo como Regression y Clustering.
 Se podría manejar estos casos listando los algoritmos dos veces o insertando
 en el grupo al que subjetivamente se ajusta mejor.
 Se utiliza este último enfoque de no duplicar algoritmos.
\end_layout

\begin_layout Subsection
Algoritmos de regresión
\end_layout

\begin_layout Standard
Los Algoritmos de Regresión (
\shape italic
Regression Algorithms
\shape default
) modelan la relación que existe entre variables, se mejora iterativamente
 utilizando una medida de error en las predicciones hechas por el modelo.
 Los métodos de regresión son herramientas de las estadísticas que se han
 adoptado en el aprendizaje de la máquina.
 Se pueden utilizar el término regresión para referirse a la clase de problema
 y también a la clase de algoritmo.
 Para ser más exactos, la regresión es realmente un proceso.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado.
\end_layout

\begin_layout Subsection
Algoritmos basados en instancia
\end_layout

\begin_layout Standard
El modelo de aprendizaje Basado en Instancia (
\shape italic
Instance-Based Learning
\shape default
) es un problema de decisión con instancias o ejemplos de datos de entrenamiento
 que se consideran importantes o requeridos para el modelo.
 Estos métodos típicamente construyen una base de datos con ejemplos y los
 compara con los nuevos datos utilizando una medida de similaridad para
 así encontrar la mejor coincidencia y hacer la predicción.
 Por esta razón, los métodos basados en la instancia también se llaman métodos
 de aprendizaje basado en memoria.
 El enfoque se pone en la representación de las instancias almacenadas y
 las medidas de similaridad utilizadas entre instancias.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de regularización
\end_layout

\begin_layout Standard
Los Algoritmos de Regularización (
\shape italic
Regularization
\shape default
), comprende una extensión hecha a otros métodos (típicamente a los métodos
 de regresión).
 Penaliza los modelos basándose en sus complejidades, favoreciendo modelos
 más simples que también son mejores de generalizar.
 Son populares, potentes y en general simples modificaciones de otros métodos.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado
 y no supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de árboles de decisión
\end_layout

\begin_layout Standard
Los métodos de Árboles de Decisión (
\shape italic
Decision Tree
\shape default
) construyen un modelo de decisiones hechas en base a los valores de atributos
 en los datos.
 Las decisiones se bifurcan en la estructura del árbol hasta que se tome
 una decisión de predicción para un registro dado.
 Los árboles de decisión son entrenados en los datos para problemas de clasifica
ción y regresión.
 Los árboles de decisión son a menudo rápidos y precisos y un gran favorito
 en aprendizaje automático.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado.
\end_layout

\begin_layout Subsection
Algoritmos bayesianos
\end_layout

\begin_layout Standard
Los métodos Bayesianos (
\shape italic
Bayesian
\shape default
) son los que aplican explícitamente el teorema de Bayes para problemas
 tales como la clasificación y la regresión.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de agrupación
\end_layout

\begin_layout Standard
La Agrupación (
\shape italic
Clustering
\shape default
) así como también la regresión describen la clase de problema y la clase
 de método.
 Los métodos de agrupación suelen estar organizados según el enfoque del
 modelado, tales como los basados en centroides y los jerárquicos.
 Todos los métodos atañen a la utilización de las estructuras inherentes
 en los datos, para organizar dichos datos de la mejor manera posible en
 grupos de máxima uniformidad.
 Estos algoritmos se implementan en problemas de aprendizaje no supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de aprendizaje de reglas de asociación
\end_layout

\begin_layout Standard
Los métodos de aprendizaje de Reglas de Asociación (
\shape italic
Rule System
\shape default
) extraen reglas que mejor explican las relaciones observadas entre variables
 en los datos.
 Estas reglas pueden descubrir asociaciones importantes y comercialmente
 útiles, en grandes conjuntos de datos multidimensionales que pueden ser
 explotados por una organización.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de redes neurales artificiales
\end_layout

\begin_layout Standard
Las Redes Neuronales artificiales (
\shape italic
Neural Networks
\shape default
) son modelos inspirados en la estructura y/o función de las redes neuronales
 biológicas.
 Son una clase de búsqueda de patrones que se utilizan comúnmente para problemas
 de regresión y clasificación.
 Es realmente un enorme subcampo compuesto de cientos de algoritmos y variacione
s para todo tipo de tipos de problemas.
 Se ha separado el aprendizaje profundo de las redes neuronales debido a
 su enorme crecimiento y popularidad.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado,
 no supervisado y por refuerzo.
\end_layout

\begin_layout Subsection
Algoritmos de aprendizaje profundo
\end_layout

\begin_layout Standard
Los métodos de Aprendizaje Profundo (
\shape italic
Deep Learning
\shape default
) son una moderna actualización de las redes neuronales artificiales que
 explotan el abundante y barato poder de computación.
 Se ocupan en construir redes neuronales mucho más grandes y complejas y,
 muchos métodos se refieren a problemas de aprendizaje semi-supervisados
 donde grandes conjuntos de datos contienen muy pocos datos etiquetados.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado
 y no supervisado.
\end_layout

\begin_layout Subsection
Algoritmos de reducción de dimensionalidad
\end_layout

\begin_layout Standard
Al igual que los métodos de agrupación, la Reducción de la Dimensionalidad
 (
\shape italic
Dimensionality Reduction
\shape default
) busca y explora la estructura inherente en los datos, pero en este caso
 de una manera no supervisada o en orden a resumir o describir los datos
 utilizando menos información.
 Esto puede ser útil para visualizar datos dimensionales o para simplificar
 datos que luego se pueden utilizar en un método de aprendizaje supervisado.
 Muchos de estos métodos pueden ser adaptados para su uso en clasificación
 y regresión.
 Estos algoritmos se implementan en problemas de aprendizaje no supervisado.
\end_layout

\begin_layout Subsection
Algoritmos ensamble
\end_layout

\begin_layout Standard
Métodos de Ensamble (
\shape italic
Ensemble
\shape default
) son modelos compuestos por múltiples modelos más débiles, que son entrenados
 independientemente y cuyas predicciones son combinadas de alguna manera
 para hacer la predicción general.
 Mucho esfuerzo se pone en qué tipos de aprendices débiles combinar y las
 formas en que hay que combinarlos.
 Esta es una clase de técnica muy poderosa y como tal es muy popular.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado
 y no supervisado.
\end_layout

\begin_layout Subsection
Algoritmos no lineales
\end_layout

\begin_layout Standard
Uno de los mas importante es Support Vector Machines.
 Estos algoritmos se implementan en problemas de aprendizaje supervisado
 y no supervisado.
\end_layout

\begin_layout Section
Algoritmos de aprendizaje por refuerzo
\end_layout

\begin_layout Standard
Se puede complementar el listado propuesto por el Dr.
 Jason con alguno de los algoritmos de aprendizaje por refuerzo.
\end_layout

\begin_layout Subsection
Programación dinámica
\end_layout

\begin_layout Standard
Estos son los principales algoritmos: 
\end_layout

\begin_layout Itemize
Programación dinámica con Policy Iteration.
\end_layout

\begin_layout Itemize
Programación dinámica con Value Iteration.
\end_layout

\begin_layout Itemize
Programación dinámica con Generalized Policy Iteration (GPI
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "GPI"
description "Generalized Policy Iteration"

\end_inset

).
\end_layout

\begin_layout Itemize
Asynchronous DP.
\end_layout

\begin_layout Itemize
Bootstrapping.
\end_layout

\begin_layout Subsection
Método de Monte Carlo
\end_layout

\begin_layout Standard
Tiene como principales algoritmos:
\end_layout

\begin_layout Itemize
On-Policy Monte Carlo Control.
\end_layout

\begin_layout Itemize
Off-Policy Monte Carlo Control.
\end_layout

\begin_layout Subsection
Aprendizaje por Diferencias Temporales
\end_layout

\begin_layout Standard
Sus principales algoritmos son:
\end_layout

\begin_layout Itemize
Sarsa: On-Policy TD Control.
\end_layout

\begin_layout Itemize
Q-learning: Off-Policy TD Control.
\end_layout

\begin_layout Itemize
Actor-Critic Methods.
\end_layout

\begin_layout Itemize
R-Learning for Undiscounted Continual Tasks.
\end_layout

\begin_layout Section
Problemas de clasificación y regresión
\end_layout

\begin_layout Subsection
Clasificación
\end_layout

\begin_layout Subsubsection
Clasificación binaria
\end_layout

\begin_layout Standard
En su forma más simple se reduce a la siguiente cuestión: dado un patrón
 
\begin_inset Formula $x$
\end_inset

 extraído de un dominio 
\begin_inset Formula $X$
\end_inset

, estimar qué valor asumirá una variable aleatoria binaria asociada 
\begin_inset Formula $y\in\left\{ \pm1\right\} $
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "IntroML2008"

\end_inset

.
\end_layout

\begin_layout Standard
La clasificación binaria es probablemente el problema más estudiado en el
 aprendizaje automático y ha dado lugar a una gran cantidad de desarrollos
 algorítmicos y teóricos importantes durante el siglo pasado.
 En su forma más simple, se reduce a la pregunta: dado un patrón x extraído
 de un dominio X, estimar qué valor asumirá una variable aleatoria binaria
 asociada y ∈ {± 1}.
\end_layout

\begin_layout Standard
Por ejemplo, si se muestran imágenes de manzanas y naranjas, podemos indicar
 si el objeto en cuestión es una manzana o una naranja.
 Igualmente bien, si se quiere predecir si un propietario de vivienda podría
 incumplir su préstamo dado sus datos de ingresos y su historial de crédito,
 o si un correo electrónico determinado es spam o no.
\end_layout

\begin_layout Subsubsection
Clasificación multiclase
\end_layout

\begin_layout Standard
Es la extensión lógica de la clasificación binaria.
 La principal diferencia es que ahora 
\begin_inset Formula $y\in\left\{ 1,2,3..,N\right\} $
\end_inset

 puede asumir un rango de valores diferentes 
\begin_inset CommandInset citation
LatexCommand cite
key "IntroML2008"

\end_inset

.
 El problema de estudio en cuestión utiliza clasificación multiclase, donde
 
\begin_inset Formula $y\in\left\{ Nada,Poco,Medio,Mucho\right\} $
\end_inset

.
 Por ejemplo, es posible que se desee clasificar un documento de acuerdo
 con el idioma en el que fue escrito (inglés, francés, alemán, español,
 hindi, japonés, chino, ...).
 
\end_layout

\begin_layout Standard
La principal diferencia es que el costo del error puede depender en gran
 medida del tipo de error que se comete.
 Por ejemplo, en el problema de evaluar el riesgo de cáncer, hace una diferencia
 significativa si clasificamos erróneamente una etapa temprana del cáncer
 como saludable (en cuyo caso es probable que el paciente muera) o una etapa
 avanzada de cáncer (en cuyo caso es probable que el paciente sufra molestias
 por un tratamiento excesivamente agresivo).
\end_layout

\begin_layout Standard
Para desarrollar un modelo o esquema de Machine Learning para resolver problemas
 de clasificación multiclase, es necesario conocer los componentes esenciales
 que la forman 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset

.
\end_layout

\begin_layout Subsubsection*
Ejemplos o instancias
\end_layout

\begin_layout Standard
La entrada de un esquema de aprendizaje automático es un conjunto de instancias.
 Estas instancias son las cosas que deben ser clasificadas, asociadas o
 agrupadas.
 En el escenario estándar, cada instancia es un ejemplo individual e independien
te del concepto que se debe aprender.
\end_layout

\begin_layout Subsubsection*
Características o atributos
\end_layout

\begin_layout Standard
Las instancias son caracterizadas mediante los valores de un conjunto predetermi
nado de atributos.
 Cada instancia proporciona una entrada al aprendizaje automático y es caracteri
zado por los valores de un conjunto fijo y predefinido de características
 o atributos.
\end_layout

\begin_layout Subsubsection*
Etiquetas
\end_layout

\begin_layout Standard
Las cantidades nominales tienen valores que son símbolos distintos.
 Los valores mismos sirven como etiquetas o nombres, de ahí el término nominal,
 que viene de la palabra latina para nombre.
 Los atributos nominales a veces se llaman categorizados, enumerados o discretos.
\end_layout

\begin_layout Subsubsection*
Conjunto de entrenamiento
\end_layout

\begin_layout Standard
El grupo de ejemplos utilizados en el proceso de entrenamiento de los algoritmos
 de aprendizaje automático constituyen el conjunto de entrenamiento.
\end_layout

\begin_layout Subsubsection*
Algoritmos de clasificación multiclase
\end_layout

\begin_layout Standard
Constituye el conjunto de algoritmos de machine learning que soportan problemas
 de clasificación multiclase.
 Cada algoritmo se construye con su Función Objetivo (f), Variables de entrada
 (X), Variable de salida (Y), donde Y = f(X)
\end_layout

\begin_layout Subsubsection*
Conjunto de prueba
\end_layout

\begin_layout Standard
Para predecir el rendimiento de un clasificador sobre nuevos datos, necesitamos
 evaluar su tasa de error en un conjunto de datos que no desempeñó ningún
 papel en la formación del clasificador.
 Este conjunto de datos independiente se denomina conjunto de prueba.
\end_layout

\begin_layout Subsection
Regression
\end_layout

\begin_layout Standard
Es otra aplicación prototípica.
 Aquí el objetivo es estimar una variable de valor real y ∈ R dado un patrón
 x.
 Por ejemplo, se podría querer estimar el valor de un stock al día siguiente,
 el rendimiento de un fabuloso semiconductor dado el proceso actual, el
 contenido de hierro de las mediciones de espectroscopia de masas dadas
 por el mineral o la frecuencia cardíaca de un atleta, dada la información
 del acelerómetro.
 Una de las cuestiones clave en las que los problemas de regresión difieren
 entre sí es la elección de una pérdida.
 Por ejemplo, al estimar los valores de stock, nuestra pérdida para una
 opción de venta será decididamente unilateral.
 Por otro lado, a un deportista aficionado solo le importaría que nuestra
 estimación de la frecuencia cardíaca coincida con la media real.
\end_layout

\begin_layout Section
Algoritmos de clasificación en Weka
\end_layout

\begin_layout Standard
Weka
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "Weka"
description "Waikato Environment for Knowledge Analysis"

\end_inset

 es una colección de algoritmos de aprendizaje automático para tareas de
 minería de datos.
 Los algoritmos pueden ser aplicados directamente a un conjunto de datos
 o llamados desde código Java.
 Weka contiene herramientas para pre-procesamiento de datos, clasificación,
 regresión, clustering, reglas de asociación y visualización.
 También es adecuado para desarrollar nuevos esquemas de aprendizaje automático
 
\begin_inset CommandInset citation
LatexCommand citet
key "Weka3"

\end_inset

.
 En el problema de estudio se utiliza el conjunto de algoritmos de clasificación
 de Weka 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2016"

\end_inset

.
 Los algoritmos de clasificación de Weka que se utilizarán son los siguientes
 
\begin_inset CommandInset citation
LatexCommand cite
key "WekaCla"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="43" columns="3">
<features islongtable="true" longtabularalignment="center">
<column alignment="left" valignment="top" width="3.5cm">
<column alignment="left" valignment="top" width="6.3cm">
<column alignment="left" valignment="top" width="5.5cm">
<row caption="true">
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Clasificadores-Weka"

\end_inset

Clasificadores Weka
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\shape italic
\size small
Categoría del clasificador
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\shape italic
\size small
Nombre del clasificador
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\shape italic
\size small
Modelo, técnica o algoritmo que implementa
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores bayesianos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
BayesNet
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Bayes Network (Red Bayesiana) 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "WekaMan3-8-0"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores bayesianos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
NaiveBayes
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Naive Bayes 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "John1995"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores bayesianos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
NaiveBayesMultinomial
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Naive Bayes multinomial 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Mccallum1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores bayesianos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
NaiveBayesMultinomialUpdateable
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Naive Bayes multinomial actualizable 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Mccallum1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores bayesianos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
NaiveBayesUpdateable
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Naive Bayes actualizable 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "John1995"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Basado en funciones
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Logistic
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Regresión Logística 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "leCessie1992"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Basado en funciones
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
MultilayerPerceptron
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Red Neuronal con 
\shape italic
back propagation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Basado en funciones
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
SimpleLogistic
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Regresión Logística lineal con LogitBoost 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Landwehr2005"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Sumner2005"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Basado en funciones
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
SMO
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SMO"
description "Sequential Minimal Optimization"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sequential Minimal Optimization con Support Vector 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Platt1998"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Keerthi2001"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Hastie1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores perezosos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
IBk
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
K-nearest neighbours (K vecinos más cercanos) 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Aha1991"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores perezosos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
KStar
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
K* con función de distancia basada en entropía 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Cleary1995"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificadores perezosos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
LWL
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LWL"
description "Locally Weighted Learning"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Locally Weighted Learning (Aprendizaje Ponderado Localmente) 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Frank2003"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Atkeson1996"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
AdaBoostM1
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Adaboost M1 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Freund1996"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
AttributeSelectedClassifier
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Selección de atributos
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Bagging
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Bagging 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Breiman1996"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
ClassificationViaRegression
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Métodos de regresión 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Frank1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
CVParameterSelection
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Selección de parámetros 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Kohavi1995"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
FilteredClassifier
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Filtro arbitrario
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
IterativeClassifierOptimizer
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Optimización del número de iteraciones
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
LogitBoost
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Regresión Logística aditiva 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Friedman1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
MultiClassClassifier
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Metaclasificador
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
MultiClassClassifierUpdateable
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Metaclasificador actualizable
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
MultiScheme
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Selección del clasificador
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
RandomCommittee
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Conjunto aleatorizado de clasificadores base
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
RandomizableFilteredClassifier
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificador arbitrario con filtro arbitrario
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
RandomSubSpace
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árbol de decisión 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Ho1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Stacking
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Combinación de clasificadores utilizando apilamiento 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Wolpert1992"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Vote
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Combinación de clasificadores 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Kuncheva2004"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Kittler1998"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Meta algoritmos
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
WeightedInstancesHandlerWrapper
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Soporte de instancias ponderadas
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sistema de reglas
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
DecisionTable
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Tabla de decisión simple 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Kohavi1995a"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sistema de reglas
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
JRip
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
\begin_inset Quotes eld
\end_inset

Repeated Incremental Pruning to Produce Error Reduction
\begin_inset Quotes erd
\end_inset

 (RIPPER) 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Cohen1995"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sistema de reglas
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
OneR
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificador 1R 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Holte1993"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sistema de reglas
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
PART
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Divide y vencerás para construir un árbol de decisión C4.5 parcial 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Frank1998a"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Sistema de reglas
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
ZeroR
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Clasificador 0-R
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
DecisionStump
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Decision stump in conjunction with a boosting algorithm
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
HoeffdingTree
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Algoritmo de inducción incremental del árbol de decisión 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Hulten2001"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
J48
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árbol de decisión C4.5 podado o no podado 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Quinlan1993"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
LMT
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LMT"
description "Logistic Model Trees"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
\begin_inset Quotes eld
\end_inset

Árboles de Modelos Logísticos
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

Logistic Model Trees
\begin_inset Quotes erd
\end_inset

 (LMT) 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Landwehr2005"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "Sumner2005"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
RandomForest
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
\begin_inset Quotes eld
\end_inset

Bosque de Árboles Aleatorios
\begin_inset Quotes erd
\end_inset

 o 
\begin_inset Quotes eld
\end_inset

Forest of Random Trees
\begin_inset Quotes erd
\end_inset

 
\size default

\begin_inset CommandInset citation
LatexCommand citet
key "Breiman2001"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
RandomTree
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Considera K atributos elegidos al azar en cada nodo.
 No realiza poda.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Árboles de decisión
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
REPTree
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size small
Construye un árbol de decisión/regresión utilizando la información de ganancia/v
arianza
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Técnicas de evaluación para aprendizaje supervisado
\end_layout

\begin_layout Standard
La evaluación es la clave para lograr avances reales en el aprendizaje automátic
o.
 Entre las técnicas de evaluación para problemas de aprendizaje supervisado
 se destacan:
\end_layout

\begin_layout Subsection
Entrenamiento y prueba sobre los mismos datos
\end_layout

\begin_layout Standard
Recompensa modelos demasiado complejos que "sobreajustan" los datos de entrenami
ento y que no necesariamente generalizan.
\end_layout

\begin_layout Subsection
Porcentaje de División
\end_layout

\begin_layout Standard
La técnica de Porcentaje de División (
\shape italic
Percentage Split
\shape default
) divide el conjunto de datos en dos partes, de modo que el modelo pueda
 ser entrenado y probado sobre diferentes datos.
 Presenta una mejor estimación del rendimiento fuera de la muestra, pero
 sigue siendo una estimación de "alta varianza".
 Útil debido a su velocidad, simplicidad y flexibilidad.
\end_layout

\begin_layout Standard
Divide el conjunto de datos en dos conjuntos: un conjunto de entrenamiento
 (generalmente el 70% del conjunto de datos completo) del que aprende el
 modelo y un conjunto de prueba (el otro 30%).
 Debido a que el conjunto de prueba se retiene del modelo durante el entrenamien
to, puede contribuir a una evaluación imparcial de qué tan bien se desempeña
 un modelo en datos nunca antes vistos.
 Esto protege contra el sobreajuste y permite evaluar cómo el modelo funcionaría
 con los nuevos datos a medida que surgen.
\end_layout

\begin_layout Subsection
Validación Cruzada de k pasadas
\end_layout

\begin_layout Standard
La técnica de Validación Cruzada de k pasadas (k-fold Cross-Validation)
 consiste en dividir los datos en un número de particiones iguales.
 En la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Esq-general-de-Cross"

\end_inset

 se muestra un ejemplo de una validación cruzada de 10 pasadas donde se
 divide el conjunto de datos en diez particiones de igual tamaño.
 Luego se entrena el modelo en nueve de esas diez particiones y se prueba
 el modelo en la partición restante.
 Luego, se repite el proceso, seleccionando una partición diferente para
 que sea el grupo de prueba y entrenando un nuevo modelo en el conjunto
 restante de nueve particiones.
 El proceso se repite ocho veces más, para un total de diez rondas de validación
 cruzada, una por cada pasada.
 Luego se tienen diez modelos diferentes, cada uno ha sido entrenado y probado
 en un subconjunto diferente de datos y cada uno tiene su propio peso y
 exactitud de predicción.
 Al final, se combinan estos modelos al promediar sus pesos para estimar
 un modelo predictivo final.
 La validación cruzada es otro antídoto contra el sobreajuste.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/Esquema K-fold Cross Validation.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Esq-general-de-Cross"

\end_inset

Esquema general de iteraciones para 10-fold Cross-Validation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La técnica estándar para predecir la tasa de error es Stratified k-fold
 Cross-Validation, donde la estratificación se refiere al proceso de reorganizar
 los datos de tal manera a asegurar que cada partición sea una buena representac
ión del conjunto.
 Comúnmente se acepta que 10 es el número de particiones con el que se obtiene
 la mejor estimación de error, idea basada en diversas pruebas sobre conjuntos
 de datos diferentes y para distintas técnicas de aprendizaje 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset

.
\end_layout

\begin_layout Section
Métricas de desempeño para problemas de clasificación 
\end_layout

\begin_layout Standard
Las métricas de clasificación son las medidas contra las cuales se evalúan
 los modelos.
 Para los problemas de clasificación, es natural medir el rendimiento de
 un clasificador en términos de la tasa de error (
\shape italic
Error Rate
\shape default
).
 El clasificador predice la clase de cada instancia: si es correcta se cuenta
 como un éxito, sino se cuenta como un error.
 La tasa de error es sólo la proporción de errores cometidos sobre un conjunto
 de instancias, y mide el rendimiento general del clasificador.
 Por supuesto, lo que interesa es el probable desempeño futuro en nuevos
 datos, no el rendimiento sobre datos pasados.
\end_layout

\begin_layout Standard
Para predecir el rendimiento de un clasificador sobre nuevos datos, necesitamos
 evaluar su tasa de error en un conjunto de datos que no desempeñó ningún
 papel en la formación del clasificador.
 Este conjunto de datos independiente se denomina conjunto de prueba.
 En tales situaciones se suele hablar de tres conjuntos de datos, los datos
 de entrenamiento, los datos de validación y los datos de prueba 
\begin_inset CommandInset citation
LatexCommand cite
key "DM2011"

\end_inset

.
 Los datos de entrenamiento son utilizados por uno o más esquemas de aprendizaje
 para conocer clasificadores.
 Los datos de validación se utilizan para optimizar los parámetros de los
 clasificadores, o para seleccionar uno determinado.
 A continuación, los datos de prueba se utilizan para calcular la tasa de
 error del método final optimizado.
 Cada uno de los tres conjuntos debe ser independiente: El conjunto de validació
n debe ser diferente del conjunto de entrenamiento para obtener un buen
 desempeño en la etapa de optimización o selección y el conjunto de prueba
 debe ser diferente de ambos para obtener una estimación confiable de la
 tasa de error real.
 
\end_layout

\begin_layout Standard
Por fines prácticos generalmente se utiliza la metodología de dividir los
 datos en dos conjuntos, el conjunto de entrenamiento y el conjunto de prueba,
 como es el caso del trabajo actual donde se utilizan valores por defecto
 en los parámetros de los clasificadores.
\end_layout

\begin_layout Subsection
Porcentaje de Acierto
\end_layout

\begin_layout Standard
Porcentaje de instancias correctamente clasificadas o de predicciones correctas.
 La métrica más simple y más común es la tasa de acierto.
 La tasa de acierto se calcula dividiendo el número de predicciones correctas
 entre el número total de predicciones.
 Por ejemplo, en un modelo de clasificación supervisada para transacciones
 bancarias, si probamos el modelo en cien transacciones y se predice correctamen
te la etiqueta (fraude/no fraude) para noventa de ellas, entonces el porcentaje
 de acierto del modelo es del 90%.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Aciertos=\frac{Nro\,Predicciones\,Correctas}{Número\,Total\,Predicciones}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Matriz de confusión
\end_layout

\begin_layout Standard
El porcentaje de acierto es la métrica de clasificación más fácil, simple
 y entendible que se puede utilizar.
 Pero no dice sobre la distribución subyacente de los valores de respuesta,
 ni dice qué "tipos" de errores está cometiendo el clasificador.
 No distingue entre falsos positivos, transacciones incorrectamente clasificadas
 como fraude y falsos negativos, transacciones incorrectamente clasificadas
 como no fraudulentas.
 Es por eso que se necesita la matriz de confusión.
\end_layout

\begin_layout Standard
Una matriz de confusión para una clasificación binaria es una tabla de 2
 x 2 como se muestra en la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:matriz-confusion"

\end_inset

 que clasifica las predicciones en una de cuatro clasificaciones: verdadero
 positivo (
\shape italic
True Positive - TP
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "TP"
description "True Positive"

\end_inset

), verdadero negativo (
\shape italic
True Negative - TN
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "TN"
description "True Negative"

\end_inset

), falso positivo (
\shape italic
False Positive - FP
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "FP"
description "False Positive"

\end_inset

) y falso negativo (
\shape italic
False Negative - FN
\shape default

\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "FN"
description "False Negative"

\end_inset

).
 En problemas de clasificación para aplicaciones reales normalmente los
 errores cuestan diferentes cantidades.
 Por ejemplo en bancos y financieras el costo de prestar a una persona que
 no paga sus deudas es mayor que el costo de rechazar un préstamo a una
 persona que es pagadora.
 Los Verdaderos Positivos y Verdaderos Negativos son clasificaciones correctas.
 Un Falso Positivo es cuando el resultado se predice incorrectamente como
 positivo cuando es realmente negativo.
 Un Falso Negativo es cuando el resultado se predice incorrectamente como
 negativo cuando es realmente positivo.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/confusion_matrix_1.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:matriz-confusion"

\end_inset

Matriz de confusión para clasificación binaria.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
En la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:matriz-confusion-1"

\end_inset

 se muestra una generalización de la matriz de confusión para clasificación
 multiclase.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/Matriz_Confusion_generica.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:matriz-confusion-1"

\end_inset

Matriz de confusión para clasificación multiclase.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La matriz de confusión brinda una imagen más completa de cómo está funcionando
 el clasificador.
 También permite calcular varias métricas de clasificación y estas métricas
 pueden guiar la selección del modelo.
 La elección de la métrica depende del objetivo de su negocio.
 Es importante identificar si FP o FN es más relevante reducir, para luego
 elegir la métrica con la variable relevante (FP o FN en la ecuación).
 Entre las métricas calculadas a partir de una matriz de confusión se encuentran
:
\end_layout

\begin_layout Subsubsection
Exactitud de clasificación (Classification accuracy)
\end_layout

\begin_layout Standard
En general, ¿con qué frecuencia es correcto el clasificador?.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Exactitud=\frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Error de clasificación (Classification error)
\end_layout

\begin_layout Standard
En general, ¿con qué frecuencia el clasificador es incorrecto?.
 También conocido como "Tasa de clasificación incorrecta".
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Error=\frac{FP+FN}{TP+TN+FP+FN}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Sensibilidad (Sensitivity)
\end_layout

\begin_layout Standard
Cuando el valor real es positivo, ¿con qué frecuencia es correcta la predicción?.
 Algo que queremos maximizar.
 ¿Qué tan "sensible" es el clasificador para detectar instancias positivas?.
 También conocido como "Tasa positiva real" o "Recordar" (True Positive
 Rate or Recall).
\end_layout

\begin_layout Standard
Todo positivo = TP + FN.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Sensibilidad=\frac{TP}{TP+FN}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Especificidad (Specificity)
\end_layout

\begin_layout Standard
Cuando el valor real es negativo, ¿con qué frecuencia es correcta la predicción?.
 Algo que queremos maximizar.
 ¿Qué tan "específico" (o "selectivo") es el clasificador en predecir las
 instancias positivas?
\end_layout

\begin_layout Standard
Todo negativo = TN + FP.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Especificidad=\frac{TN}{TN+FP}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Tasa de falsos positivos (False positive rate)
\end_layout

\begin_layout Standard
Cuando el valor real es negativo, ¿con qué frecuencia la predicción es incorrect
a?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Tasa\:de\:falsos\:positivos=\frac{FP}{TN+FP}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Precisión (Precision)
\end_layout

\begin_layout Standard
Cuando se predice un valor positivo, ¿con qué frecuencia es correcta la
 predicción?.
 ¿Cuán "preciso" es el clasificador al predecir instancias positivas?.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Presición=\frac{TP}{TP+FP}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Medida F (F-measure)
\end_layout

\begin_layout Standard
La Medida F es una medida de la exactitud de una prueba.
 La Medida F puede interpretarse como un promedio ponderado de la precisión
 y sensibilidad, donde alcanza su mejor valor en 1 y el peor en 0.
 Se define como:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Medida\:F=\frac{2*Sensibilidad*Precision}{\left(Sensibilidad+Precision\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection

\color blue
Estadística Kappa (Kappa Statistic)
\end_layout

\begin_layout Standard
En la predicción multiclase, cada elemento de la matriz de confusión muestra
 el número de ejemplos de prueba para los que la clase real es la fila y
 la clase prevista es la columna.
 Son buenos resultados los grandes números en la diagonal principal e idealmente
 cero fuera de la diagonal principal.
 
\begin_inset Quotes eld
\end_inset

Kappa se utiliza para medir el acuerdo entre la predicción y la observación
 de las categorizaciones de un conjunto de datos, mientras que se corrige
 para un acuerdo que ocurre por casualidad
\begin_inset Quotes erd
\end_inset

.
 Si los evaluadores están totalmente de acuerdo Kappa alcanza un valor máximo
 igual a 1.
 Si no hay total acuerdo entre los evaluadores, entonces Kappa tiene un
 valor 
\begin_inset Formula $<1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\kappa=\frac{Pr\left(a\right)-Pr\left(e\right)}{1-Pr\left(e\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Donde: 
\begin_inset Formula $Pr\left(a\right)$
\end_inset

 es el acuerdo observado relativo entre los observadores y 
\begin_inset Formula $Pr\left(e\right)$
\end_inset

 es la probabilidad hipotética de acuerdo al azar utilizando los datos observado
s para calcular las probabilidades de que cada observador clasifique aleatoriame
nte cada categoría.
\end_layout

\begin_layout Standard
—-
\end_layout

\begin_layout Standard
Métricas de error para clases sesgadas (skewed classes): - Precisión/Recuperació
n (Precision/Recall): Tabla: Clase Real x Clase Predicha.
 Positivo Verdadero (la clase predicha es 1 y la clase real es 1), Negativo
 Verdadero (la clase predicha es 0 y la clase real es 0), Positivo Falso
 (la clase predicha es 1 y la clase real es 0), Negativo False (la clase
 predicha es 0 y la clase real es 1) - Precision = Positivos Verdaderos
 / #Positivos predichos = Positivos Verdaderos / (Positivos Verdaderos +
 Positivos Falsos).
 De todos los pacientes a quienes se predice 1 qué fraccion tiene verdaderamente
 la enfermedad.
 Una precisión alta es deseada.
 - Recall = Positivos Verdaderos / #Positivos reales = Positivos Verdaderos
 / (Positivos Verdaderos + Negativos Falsos).
 De todos los pacientes que tienen la enfermedad, que fracción fue correctamente
 detectada que tiene la enfermedad.
 Un recall alto será bueno.
 Por ejemplo si todo predice 0 entonces recall = 0, y es un indicador de
 que no es un buen clasificador.
 - Sirve aún para las situaciones más sesgadas.
 - Métrica de evaluación útil y directa para entender si el algoritmo se
 está desempeñando bien.
 No permite que el algoritmo haga trampa.
 - Convención: y = 1 en presencia de la clase más rara.
\end_layout

\begin_layout Standard
Compensación entre Precisión y Recuperación (trading off Precision and Recall):
 - Predice 1 si h(x) mayor o igual a 0,7.
 Alta precisión, bajo recall.
 Para evitar Positivos Falsos.
 - Predice 1 si h(x) mayor o igual a 0.3.
 Baja precisión, alto recall.
 Para evitar Negativos Falsos.
 - Generalizando: Predice 1 si h(x) mayor o igual a un UMBRAL.
 Se traza la curva de Recall (Eje x) y Precision (Eje y).
 - Cómo compara los valores numéricos de Precisión y Recall.
 F o F1 Score = 2(PR/P+R).
 El mayor valor posible se obtiene para Precisión = 1 y Recall = 1.
 El más bajo posible se obtiene para Precisión = 0 y Recall = 0.
 - Se controla la Precisión y el Recall variando el UMBRAL.
 - Se puede intentar probar un rango de valores para el UMBRAL.
 Se prueba en el conjunto de validación cruzada.
 Luego se elige el UMBRAL que arroje el valor de F1 más alto.
 Esto es una forma de elegir de forma automática el UMBRAL para el clasificador.
\end_layout

\begin_layout Standard
—
\end_layout

\begin_layout Section
Componentes de los errores de predicción: Ruido, Sesgo y Varianza
\end_layout

\begin_layout Standard
Es importante evaluar el rendimiento de un modelo de aprendizaje de la mejor
 manera posible.
 Una de las razones es para determinar si usar o no el modelo para la predicción
 de valores futuros, otra de las razones es que la evaluación es una componente
 integral de muchos métodos de aprendizaje.
 Los errores de predicción en los modelos de aprendizaje automático tienen
 tres componentes: Ruido (
\shape italic
Noise
\shape default
), Sesgo (
\shape italic
Bias
\shape default
) y Varianza (
\shape italic
Variance
\shape default
)
\end_layout

\begin_layout Subsection
Ruido
\end_layout

\begin_layout Standard
El ruido es un error aleatorio de varianza de la variable que está siendo
 medida 
\begin_inset CommandInset citation
LatexCommand cite
key "Han2011"

\end_inset

, representa el componente 
\begin_inset Quotes eld
\end_inset

irreducible
\begin_inset Quotes erd
\end_inset

 de error de generalización en una función, este término está fuera del
 control incluso aún conociendo la verdadera función de predicción.
 El ruido es una distorsión de los datos originales, está presente en casi
 cualquier problema del mundo real, sin embargo no siempre son conocidas.
 Como ejemplo de ruido en los datos para un sistema de monitoreo ambiental
 puede darse por mal funcionamientos, calibraciones defectuosas en sensores
 de medición, problemas de la red que transporta los datos 
\begin_inset CommandInset citation
LatexCommand cite
key "Kalapanidas2003"

\end_inset

.
\end_layout

\begin_layout Subsection
Sesgo
\end_layout

\begin_layout Standard
La precisión observada de la hipótesis aprendida a través de los ejemplos
 de entrenamiento es a menudo una mala estimación de su exactitud sobre
 futuros ejemplos.
 Debido a que la hipótesis aprendida se derivó de estos ejemplos, típicamente
 proporcionarán una estimación optimista sesgada de la precisión de la hipótesis
 sobre ejemplos futuros, esto se conoce como sesgo en la estimación 
\begin_inset CommandInset citation
LatexCommand cite
key "Mitchell1997"

\end_inset

.
 El sesgo conocido también como 
\begin_inset Quotes eld
\end_inset

subajuste
\begin_inset Quotes erd
\end_inset

, ocurre cuando la función de aprendizaje mapea pobremente los datos de
 entrenamiento y esto da un error de generalización con datos futuros.
 En estadística se entiende por sesgo como la diferencia al cuadrado entre
 el verdadero valor y el valor esperado de la estimación 
\begin_inset CommandInset citation
LatexCommand cite
key "Friedman2001"

\end_inset

.
 El sesgo define la capacidad de ajuste del modelo de la función de aprendizaje,
 en la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Errores-en-la-prediccion"

\end_inset

se observa que a menor complejidad, mayor sesgo, esto indica que el error
 de predicción tanto en el entrenamiento como en el testeo es mayor cuando
 el modelo seleccionado es mas simple 
\begin_inset CommandInset citation
LatexCommand cite
key "Dietterich1995"

\end_inset

.
\end_layout

\begin_layout Subsection
Varianza
\end_layout

\begin_layout Standard
Si la precisión de la hipótesis se mide en un conjunto imparcial de ejemplos
 de prueba independientes de los ejemplos de entrenamiento, la precisión
 medida puede variar de la precisión real, dependiendo de la configuración
 del conjunto particular de ejemplos de prueba.
 Cuanto menor es el conjunto de ejemplos de prueba, mayor es la varianza
 esperada 
\begin_inset CommandInset citation
LatexCommand cite
key "Mitchell1997"

\end_inset

.
 La varianza llamada también 
\begin_inset Quotes eld
\end_inset

sobreajuste
\begin_inset Quotes erd
\end_inset

 se da cuando la función seleccionada aprende casi perfectamente la tendencia
 de los datos del ejemplo de entrenamiento, pero falla en la generalización
 con datos nuevos del futuro.
 La varianza es una medida de generalización, tanto el sesgo como la varianza
 están relacionadas con la complejidad del modelo de la función, la figura
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Errores-en-la-prediccion"

\end_inset

 muestra que a medida que aumenta la complejidad del modelo, la varianza
 tiende a aumentar y el sesgo tiende a disminuir 
\begin_inset CommandInset citation
LatexCommand cite
key "Friedman2001"

\end_inset

.
 En Machine Learning es esencial encontrar modelos con un balance entre
 sesgo y varianza, o visto de otra forma, un equilibrio entre error y complejida
d.
\end_layout

\begin_layout Section
Complejidad del modelo y Curvas de aprendizaje
\end_layout

\begin_layout Standard
La capacidad de generalización está determinada principalmente por dos factores:
\end_layout

\begin_layout Itemize
La complejidad del modelo (model complexity).
\end_layout

\begin_layout Itemize
El tamaño del conjunto de entrenamiento (learning curves).
\end_layout

\begin_layout Standard
La figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Errores-en-la-prediccion"

\end_inset

 muestra la relación existente entre la complejidad del modelo (eje x) y
 el error de predicción (eje y).
 Indica cómo va variando el error de predicción en los conjuntos de entrenamient
o y de prueba a medida que aumenta la complejidad del modelo.
 Los medelos muy simples dan una situación no deseada de sesgo (subajuste)
 y los modelos muy complejos igualmente una situación no deseada de varianza
 (sobreajuste).
 Para obtener una buena capacidad de generalización se debe encontrar un
 compromiso entre sesgo y varianza.
\end_layout

\begin_layout Standard
Este compromiso entre sesgo y varianza se puede encontrar probando distintas
 configuraciones de los parámetros de ajuste (tuning parameters) de los
 algoritmos de aprendizaje y luego eligiendo el que arroje mejores resultados.
 Normalmente se recomienda probar las distintas configuraciones de los parámetro
s sobre un conjunto de datos aparte llamado de validación, que no se debe
 confundir con los conjuntos de entrenamiento y de prueba.
\end_layout

\begin_layout Standard
Algunos ejemplos de familias de algoritmos y sus parámetros de ajuste son:
\end_layout

\begin_layout Itemize
Algoritmos de regresión lineal: la cantidad de atributos del modelo (que
 determinará la cantidad de variables independientes).
\end_layout

\begin_layout Itemize
Modelos bayesianos: en bayesian network el algoritmo estimador (estimating
 the conditional probability tables of a Bayes network) y el método para
 encontrar estructuras de red.
\end_layout

\begin_layout Itemize
Árboles de decisión: el factor de confianza de la poda, mínimo número de
 instancias, número de pasadas para reducir el error de poda 
\begin_inset CommandInset citation
LatexCommand cite
key "Mantovani2016"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Redes neuronales artificiales: la topología de la red, donde se debe elegir
 la cantidad neronas para la capa de entrada, salida y la oculta.
 También se deben probar distintas tasas de aprendizaje (learning rate)
 y funciones de activación.
\end_layout

\begin_layout Itemize
Support vector machines: los parámetros gamma (γ), costo (C), y epsilon
 (ε).
 γ es una constante que reduce el espacio modelo y controla la complejidad
 de la solución, C es una constante positiva que es un parámetro de control
 de capacidad, mientras que ε es la función de pérdida que describe el vector
 de regresión sin todos los datos de entrada 
\begin_inset CommandInset citation
LatexCommand cite
key "Mouatadid2016"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/Bias_Varianza.PNG
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Errores-en-la-prediccion"

\end_inset

Complejidad del modelo 
\begin_inset CommandInset citation
LatexCommand cite
key "Friedman2001"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Curvas-de-aprendizaje"

\end_inset

 muestra la relación existente entre la el tamaño del conjunto de entrenamiento
 (eje x) y el error de predicción (eje y).
 Indica cómo va variando el error de predicción en los conjuntos de entrenamient
o y de prueba a medida que aumenta el tamaño del conjunto de entrenamiento.
 Los medelos muy simples dan problema de alto sesgo (figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Curvas-de-aprendizaje"

\end_inset

 (a)) y los modelos muy complejos dan problema de alta varianza (figura
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Curvas-de-aprendizaje"

\end_inset

 (b)).
\end_layout

\begin_layout Standard
Cuando el modelo está sesgado (figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Curvas-de-aprendizaje"

\end_inset

 (a)) por mas que se aumenten los datos del conjunto de entrenamiento el
 nivel de error del conjunto de entrenamiento y el de prueba se mantienen
 alto, por lo que para modelos sesgados no es una solución aumentar significativ
amente la cantidad de los datos del conjunto de entrenamiento.
 La solución se debería buscar aumentando la complejidad del modelo.
\end_layout

\begin_layout Standard
Cuando el modelo tiene alta varianza (figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Curvas-de-aprendizaje"

\end_inset

 (b)) el error en el conjunto de entrenamiento se mantiene bajo aún aumentando
 su cantidad de datos.
 El conjunto de prueba en cambio inicia con alto porcentaje de error con
 pocos datos, pero a medida que aumentan los datos en el conjunto de entrenamien
to el error del conjunto de prueba disminuye significativamente.
 Por tanto, anta alta varianza es una posible solución aumentar el tamaño
 de los datos.
 También se puede probar disminuyendo la complejidad del modelo.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename recursos/Curvas de aprendizaje2.png

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Curvas-de-aprendizaje"

\end_inset

Curvas de aprendizaje
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Trabajos relacionados a Pronóstico - Business Intelligence - Machine Learning
\end_layout

\begin_layout Standard
En esta sección se pretende dar a conocer un análisis de los trabajos relacionad
os con los tres puntos más importantes del este trabajo.
 En los textos revisados cabe destacar que no se ha encontrado un trabajo
 o estudio que aborda el problema del Pronóstico de la Demanda con Business
 Intelligence y Machine Learning en la misma solución.
 Sin embargo, publicaciones que comprenden los últimos años presentan trabajos
 muy interesantes en el ámbito de Machine Learning asociado al Pronóstico
 de la Demanda (
\shape italic
Demand Forecasting
\shape default
), como así también Pronóstico de la demanda en conjunto con Business Intelligen
ce.
 Se citan a continuación algunos de los problemas afrontados en publicaciones,
 que dan una idea del estado del arte en este tema:
\end_layout

\begin_layout Standard
En el año 2016 
\begin_inset CommandInset citation
LatexCommand cite
key "Castro2016"

\end_inset

 se afronta el problema de mejorar el tiempo y la calidad en la toma de
 decisiones gerenciales en la empresa 
\begin_inset Quotes eld
\end_inset

Figueri SRL
\begin_inset Quotes erd
\end_inset

, el tiempo y el esfuerzo requerido por el área de sistemas para la generación
 de informes a la gerencia afectan en la toma de decisiones.
 El enfoque adoptado para la solución consiste en la implementación de un
 sistema de Business Intelligence basado en un modelo de pronóstico de ventas.
 En el desarrollo de Business Intelligence se construye un Data mart bajo
 la metodología de Kimball para guardar los datos históricos, se realiza
 el proceso ETL para obtener y cargar los datos al dataware.
 El desarrollo del pronóstico se basa en el modelo de series de tiempo autorregr
esivo ARMA de la demanda de productos.
 Como resultado de la solución propuesta la empresa obtuvo una disminución
 en el porcentaje de devolución y el porcentaje de rotación de productos,
 los pronósticos ayudaron a la toma de decisiones, generando mayores ventas
 y nuevas estrategias de marketing.
\end_layout

\begin_layout Standard
En el trabajo presentado en el año 2016 
\begin_inset CommandInset citation
LatexCommand cite
key "Mouatadid2016"

\end_inset

 los autores abordan cómmo pronosticar la demanda de agua urbana en la ciudad
 de Montreal (Canadá) en plazos de 1 y 3 días.
 Para llegar a la solución propuesta se compararon cuatro modelos de pronóstico:
 Artificial Neural Network (ANN), Support Vector Regression (SVR) y Extreme
 Learning Machine (ELM) y el tradicional Multiple Linear Regression (MLR).
 Los modelos se basaron en combinaciones de variables de entrada como la
 temperatura diaria máxima, precipitación diaria total y la demanda diaria
 de agua con datos disponibles desde 1999 a 2010.
 Los dos índices de rendimiento utilizados para evaluar los modelos MLR,
 ANN, SVR y ELM fueron el Coeficiente de Determinación (
\begin_inset Formula $R^{2}$
\end_inset

) y el Error Cuadrático Medio (RMSE).
 ELM alcanzó una mayor precisión en ambos períodos de pronóstico 1 y 3 días,
 en comparación con MLR, ANN y SVR que alcanzaron buena precisión para el
 plazo de 1 día.
 En general, ELM resulta ser un método de aprendizaje eficiente cuando se
 trata de pronosticar a corto plazo.
\end_layout

\begin_layout Standard
En el año 2015 se presentó 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferreira2015"

\end_inset

 un caso de estudio basado en la empresa de retailer online 
\shape italic
Rue La La
\shape default
 (http://www.ruelala.com).
 Los autores buscan estrategias para estimar la demanda de estilos nunca
 antes vendidos y tambien buscan un algoritmo que optimize combinaciones
 de precios para que sirva como herramienta de apoyo diario en la toma de
 decisiones de precios y que maximice los ingresos de dichos primeros estilos
 de exposición.
 Los datos de transacciones de ventas abarcaron desde principios de 2011
 hasta mediados de 2013, donde cada registro representa una venta con su
 sello de tiempo, cantidad vendida del artículo, precio, fecha/hora de inicio
 del evento, duración del evento y el inventario inicial del artículo.
 Además, se disponen de datos relacionados al producto como la marca, el
 tamaño, el color, el precio sugerido por el fabricante (MSRP o Manufacturer’s
 Suggested Retail Price) y la clasificación jerárquica del producto.
 Los modelos probados fueron: Least Squares Regression, Principal Components
 Regression, Partial Least Squares Regression, Multiplicative (power) Regression
, Semilogarithmic Regression y Regression Trees.
 Para comparar estos modelos se dividió aleatoriamente los datos en conjuntos
 de entrenamiento y prueba, y se utilizó los datos de entrenamiento para
 construir los modelos de regresión.
 Para los modelos que requerían parámetros de ajuste se utilizó validación
 cruzada de cinco pasadas sobre los datos de entrenamiento.
 Se encontró que Regression Trees con Bagging superó a los demás métodos
 de regresión.
 Luego de poner en producción la herramienta las ventas no disminuyen debido
 a la implementación de aumentos de precios recomendados por el algoritmo
 de optimización de precios.
 Se logró un aumento en los ingresos del grupo de prueba en aproximadamente
 9.7%.
\end_layout

\begin_layout Standard
El trabajo presentado en el año 2014 
\begin_inset CommandInset citation
LatexCommand cite
key "HybridSoft2014"

\end_inset

, plantea como objetivo pronosticar la demanda del petróleo crudo importado
 (Imported Crude Oil - ICO) en Taiwán.
 Para el estudio se utilizaron datos reales, desde los años 1993 a 2010,
 que fueron registrado por la Oficina de Energía de Taiwán.
 Como solución se propone un modelo híbrido de dos etapas.
 Para evidenciar la efectividad de los modelos híbridos se lo comparan con
 modelos de una sola etapa.
 Los modelos de pronóstico de una sola etapa incluyen: Multiple Linear Regressio
n (MLR), Support Vector Regression (SVR), Artificial Neural Networks (ANN),
 y Extreme Learning Machine (ELM), y una selección de 23 variables influyentes
 como posibles datos de entrada.
 Para ANN y SVR se utilizaron las 23 variables, para ELM entre 1 y 15 variables,
 y para MLR se utilizaron 2 variables significativas.
 Para los modelos de pronóstico híbridos se utilizaron las 2 variables seleccion
adas para MLR como variables de entrada, dando así 3 modelos híbridos: MLR(sel)-
ANN, MLR(sel)-SVR y MLR(sel)-ELM.
 Como medidas de rendimiento de los pronósticos se utilizaron: Mean Absolute
 Percentage Error (MAPE), Root Mean Square Error (RMSE), y el Mean Absolute
 Difference (MAD) fueron utilizados.
 Los resultados mostraron que los enfoques híbridos propuestos son más precisos
 que los de una sola etapa, por lo tanto son capaces de predecir con mayor
 precisión la demanda de petróleo crudo en Taiwán.
\end_layout

\end_body
\end_document
